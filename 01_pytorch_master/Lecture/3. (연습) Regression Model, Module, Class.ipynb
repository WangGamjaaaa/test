{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 선형회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 데이터 전처리, ** 중요 딥러닝 모델의 입력 shape와 출력 shape를 확인\n",
    "2. 데이터셋 클래스 작성\n",
    "3. 데이터셋 인스턴스를 활용해서 데이터 로더 할당\n",
    "4. 딥러닝 모델 작성\n",
    "5. 순전파 함수를 정의\n",
    "6. 손실함수 정의\n",
    "7. 최적화 기법 설정\n",
    "8. 하이퍼 파라미터를 설정\n",
    "9. 학습, 검증 시행\n",
    "10. 테스트 데이터로 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader # 데이터로더\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 옵티마이저 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1532/3046418788.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimaizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moptimaizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# optimizer를 설정하기 위해서는 모델의 파라미터, lr이 필요하다\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "optimaizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimaizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# optimizer를 설정하기 위해서는 모델의 파라미터, lr이 필요하다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 손실함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수를 정의\n",
    "bce_loss = nn.BCELoss()\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "mse_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0600)\n"
     ]
    }
   ],
   "source": [
    "# 모델이 있고 예측값이 나왔다고 가정\n",
    "y_hat = torch.tensor([1.1,2.4,3.1])\n",
    "y_target = torch.tensor([1,2,3])\n",
    "# 간단하게 로스 함수의 입력에 타깃 데이토와 예측 값을 넣으면 로스가 계산된다.\n",
    "loss1 = mse_loss(y_hat,y_target)\n",
    "print(loss1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4982)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "output = torch.rand([4,10]) # 2x10 행렬을 생성합니다. 2는 데이터의 수, 10은 클레스의 수\n",
    "target = torch.LongTensor([1,9,2,3]) # 1과 9는 실제 정답\n",
    "loss2 = ce_loss(output, target)\n",
    "print(loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6614, 0.2669, 0.0617])\n",
      "tensor([0.6596, 0.5663, 0.5154])\n",
      "tensor([1., 0., 0.])\n",
      "tensor(0.6587)\n"
     ]
    }
   ],
   "source": [
    "# binary cross entropy\n",
    "torch.manual_seed(1)\n",
    "sigmoid = nn.Sigmoid()\n",
    "inp = torch.randn(3)\n",
    "y_pred = sigmoid(inp)\n",
    "target = torch.empty(3).random_(2)\n",
    "\n",
    "loss1 = bce_loss(y_pred,target)\n",
    "print(inp, y_pred, target, loss1,sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 선형회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn 모듈에는 머신러닝_딥러닝 모델을 구축하기 위해 필요한 모든 요소가 군현되어 있따.\n",
    "# 우리는 이것들을 잘 가져다 쓰기만 하면 된다.\n",
    "\n",
    "# y = wx + b\n",
    "\n",
    "linear_model = nn.Linear(1,1) # 첫번째 인자 : 입력의 차원, 두번째 인자 : 출력의 차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 선형회귀 데이터 생성\n",
    "num_data = 1000\n",
    "\n",
    "# torch.init_num_threads\n",
    "# init 모듈은 가중치나 텐서 데이터들의 분포를 초기화할 때 사용합니다.\n",
    "x = init.uniform_(torch.Tensor(num_data,1), -10,10) # 첫번째 인자 : 텐서 shape, 두번째, 세번째: 범위\n",
    "noise = init.normal_(torch.FloatTensor(num_data,1),std=1)\n",
    "y = 2*x +3\n",
    "y_noise = y+noise\n",
    "\n",
    "optimizer = optim.SGD(linear_model.parameters(), lr=0.001)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x,y)\n",
    "plt.show()\n",
    "plt.scatter(x,y_noise)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.0493]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.3652], requires_grad=True)\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(linear_model.weight, linear_model.bias, linear_model.weight.grad,linear_model.bias.grad,sep='\\n')\n",
    "# grad = tape.gradient(loss, my_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 지도학습 모델의 학습 순서  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 옵티마이저의 그래디언트를 0으로 \n",
    "2) 데이터를 모델에 넣어서 값을 예측한다.\n",
    "3) 정답 데이터와 예측값을 통해 손실함수를 계산한다.\n",
    "4) 지농미분함수인 .backward()를 이용해 그래디언트를 계산한다.\n",
    "5) 옵티마이저의 step()함수를 호출한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26.5740)\n",
      "tensor(7.0104)\n",
      "tensor(2.2222)\n",
      "tensor(1.0403)\n",
      "tensor(0.7390)\n",
      "tensor(0.6530)\n",
      "tensor(0.6201)\n",
      "tensor(0.6006)\n",
      "tensor(0.5849)\n",
      "tensor(0.5704)\n",
      "tensor(0.5567)\n",
      "tensor(0.5436)\n",
      "tensor(0.5310)\n",
      "tensor(0.5190)\n",
      "tensor(0.5073)\n",
      "tensor(0.4962)\n",
      "tensor(0.4854)\n",
      "tensor(0.4751)\n",
      "tensor(0.4652)\n",
      "tensor(0.4557)\n",
      "tensor(0.4466)\n",
      "tensor(0.4378)\n",
      "tensor(0.4294)\n",
      "tensor(0.4213)\n",
      "tensor(0.4135)\n",
      "tensor(0.4060)\n",
      "tensor(0.3988)\n",
      "tensor(0.3919)\n",
      "tensor(0.3852)\n",
      "tensor(0.3788)\n",
      "tensor(0.3727)\n",
      "tensor(0.3668)\n",
      "tensor(0.3612)\n",
      "tensor(0.3557)\n",
      "tensor(0.3505)\n",
      "tensor(0.3455)\n",
      "tensor(0.3406)\n",
      "tensor(0.3360)\n",
      "tensor(0.3316)\n",
      "tensor(0.3273)\n",
      "tensor(0.3232)\n",
      "tensor(0.3192)\n",
      "tensor(0.3154)\n",
      "tensor(0.3118)\n",
      "tensor(0.3083)\n",
      "tensor(0.3049)\n",
      "tensor(0.3017)\n",
      "tensor(0.2985)\n",
      "tensor(0.2956)\n",
      "tensor(0.2927)\n",
      "tensor(0.2899)\n",
      "tensor(0.2873)\n",
      "tensor(0.2847)\n",
      "tensor(0.2823)\n",
      "tensor(0.2799)\n",
      "tensor(0.2777)\n",
      "tensor(0.2755)\n",
      "tensor(0.2734)\n",
      "tensor(0.2714)\n",
      "tensor(0.2695)\n",
      "tensor(0.2676)\n",
      "tensor(0.2659)\n",
      "tensor(0.2641)\n",
      "tensor(0.2625)\n",
      "tensor(0.2609)\n",
      "tensor(0.2594)\n",
      "tensor(0.2580)\n",
      "tensor(0.2566)\n",
      "tensor(0.2552)\n",
      "tensor(0.2539)\n",
      "tensor(0.2527)\n",
      "tensor(0.2515)\n",
      "tensor(0.2503)\n",
      "tensor(0.2492)\n",
      "tensor(0.2482)\n",
      "tensor(0.2472)\n",
      "tensor(0.2462)\n",
      "tensor(0.2453)\n",
      "tensor(0.2444)\n",
      "tensor(0.2435)\n",
      "tensor(0.2427)\n",
      "tensor(0.2419)\n",
      "tensor(0.2411)\n",
      "tensor(0.2403)\n",
      "tensor(0.2396)\n",
      "tensor(0.2390)\n",
      "tensor(0.2383)\n",
      "tensor(0.2377)\n",
      "tensor(0.2371)\n",
      "tensor(0.2365)\n",
      "tensor(0.2359)\n",
      "tensor(0.2354)\n",
      "tensor(0.2349)\n",
      "tensor(0.2344)\n",
      "tensor(0.2339)\n",
      "tensor(0.2334)\n",
      "tensor(0.2330)\n",
      "tensor(0.2326)\n",
      "tensor(0.2322)\n",
      "tensor(0.2318)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "for i in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = linear_model(x)\n",
    "    loss = mse_loss(y_pred, target)\n",
    "    loss.backward() # 4\n",
    "    optimizer.step() # 5\n",
    "    if i % 10 == 0:\n",
    "        print(loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.0346e-05]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.3376], requires_grad=True)\n",
      "tensor([[0.]])\n",
      "tensor([0.])\n",
      "tensor([[0.]]) tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "print(linear_model.weight, linear_model.bias, linear_model.weight.grad,linear_model.bias.grad,sep='\\n')\n",
    "optimaizer.zero_grad()\n",
    "print(linear_model.weight.grad,linear_model.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 퀴즈 (Easy)  \n",
    "위 모델은 y = 2x + 3 으로 학습되지 않았습니다.  \n",
    "이를 개선하기 위해서는 하이퍼 파라미터를 조정해야 합니다.  \n",
    "1) 위 선형회귀모델에서 하이퍼파라미터의 종류는 뭐가 있을까요?  \n",
    "2) 하이퍼파라미터를 조정해서 모델의 가중치와 편향이 정답에 가깝도록 학습시켜보세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 다중선형회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_shape : torch.Size([1000, 1])\n"
     ]
    }
   ],
   "source": [
    "# 선형회귀 데이터 생성\n",
    "num_data = 1000\n",
    "\n",
    "x = init.uniform_(torch.Tensor(num_data, 3), -10, 10) # 첫번째 인자 : 텐서 shape, 두번째\n",
    "noise = init.normal_(torch.FloatTensor(num_data,1), std=1)\n",
    "weights = torch.tensor([2.,3.,1.])\n",
    "y = x.matmul(weights) + -1\n",
    "y = y.unsqueeze(1)\n",
    "y_noise = y + noise\n",
    "print(f\"y_shape : {noise.shape}\")\n",
    "# print(y)\n",
    "# print(y_noise)\n",
    "# num_data = 1000\n",
    "# num_epoch = 2000\n",
    "\n",
    "# x = init.uniform_(torch.Tensor(10, 3), -10, 10)\n",
    "# noise = init.normal_(torch.FloatTensor(10, 1), std=1)\n",
    "# weights = torch.tensor([2., 3., 1.])\n",
    "# print(x.matmul(weights).shape)\n",
    "# y = x.matmul(weights) + -1\n",
    "# y = y.unsqueeze(1)\n",
    "# print(f\"y.shape {y.shape}\")\n",
    "# y_noise = y + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_model = nn.Linear(3,1)\n",
    "optimaizer = optim.SGD(multi_model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${y = w_1 x_1 + w_2 x_2 + w_3 x_3 + b}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1000x3 and 1x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1532/1616278392.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0moptimaizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Working/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Working/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Working/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1000x3 and 1x1)"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "for i in range(num_epochs):\n",
    "    optimaizer.zero_grad()\n",
    "    y_pred = linear_model(x)\n",
    "    loss = mse_loss(y_pred, target)\n",
    "    loss.backward() # 4\n",
    "    optimaizer.step() # 5\n",
    "    if i % 10 == 0:\n",
    "        print(loss.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 퀴즈 (Easy)  \n",
    "1) 하이퍼파라미터를 조정해서 모델의 가중치와 편향이 정답에 가깝도록 학습시켜보세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 로지스틱 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[0, 2], [1, 2], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = nn.Linear(2,1)\n",
    "sigmoid = nn.Sigmoid()\n",
    "optimizer = optim.SGD(linear_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3458)\n",
      "tensor(0.3457)\n",
      "tensor(0.3456)\n",
      "tensor(0.3455)\n",
      "tensor(0.3455)\n",
      "tensor(0.3454)\n",
      "tensor(0.3453)\n",
      "tensor(0.3452)\n",
      "tensor(0.3452)\n",
      "tensor(0.3451)\n",
      "tensor(0.3450)\n",
      "tensor(0.3449)\n",
      "tensor(0.3449)\n",
      "tensor(0.3448)\n",
      "tensor(0.3447)\n",
      "tensor(0.3446)\n",
      "tensor(0.3446)\n",
      "tensor(0.3445)\n",
      "tensor(0.3444)\n",
      "tensor(0.3443)\n",
      "tensor(0.3443)\n",
      "tensor(0.3442)\n",
      "tensor(0.3441)\n",
      "tensor(0.3440)\n",
      "tensor(0.3440)\n",
      "tensor(0.3439)\n",
      "tensor(0.3438)\n",
      "tensor(0.3437)\n",
      "tensor(0.3437)\n",
      "tensor(0.3436)\n",
      "tensor(0.3435)\n",
      "tensor(0.3435)\n",
      "tensor(0.3434)\n",
      "tensor(0.3433)\n",
      "tensor(0.3432)\n",
      "tensor(0.3432)\n",
      "tensor(0.3431)\n",
      "tensor(0.3430)\n",
      "tensor(0.3429)\n",
      "tensor(0.3429)\n",
      "tensor(0.3428)\n",
      "tensor(0.3427)\n",
      "tensor(0.3426)\n",
      "tensor(0.3426)\n",
      "tensor(0.3425)\n",
      "tensor(0.3424)\n",
      "tensor(0.3423)\n",
      "tensor(0.3423)\n",
      "tensor(0.3422)\n",
      "tensor(0.3421)\n",
      "tensor(0.3421)\n",
      "tensor(0.3420)\n",
      "tensor(0.3419)\n",
      "tensor(0.3418)\n",
      "tensor(0.3418)\n",
      "tensor(0.3417)\n",
      "tensor(0.3416)\n",
      "tensor(0.3415)\n",
      "tensor(0.3415)\n",
      "tensor(0.3414)\n",
      "tensor(0.3413)\n",
      "tensor(0.3412)\n",
      "tensor(0.3412)\n",
      "tensor(0.3411)\n",
      "tensor(0.3410)\n",
      "tensor(0.3410)\n",
      "tensor(0.3409)\n",
      "tensor(0.3408)\n",
      "tensor(0.3407)\n",
      "tensor(0.3407)\n",
      "tensor(0.3406)\n",
      "tensor(0.3405)\n",
      "tensor(0.3404)\n",
      "tensor(0.3404)\n",
      "tensor(0.3403)\n",
      "tensor(0.3402)\n",
      "tensor(0.3401)\n",
      "tensor(0.3401)\n",
      "tensor(0.3400)\n",
      "tensor(0.3399)\n",
      "tensor(0.3399)\n",
      "tensor(0.3398)\n",
      "tensor(0.3397)\n",
      "tensor(0.3396)\n",
      "tensor(0.3396)\n",
      "tensor(0.3395)\n",
      "tensor(0.3394)\n",
      "tensor(0.3393)\n",
      "tensor(0.3393)\n",
      "tensor(0.3392)\n",
      "tensor(0.3391)\n",
      "tensor(0.3391)\n",
      "tensor(0.3390)\n",
      "tensor(0.3389)\n",
      "tensor(0.3388)\n",
      "tensor(0.3388)\n",
      "tensor(0.3387)\n",
      "tensor(0.3386)\n",
      "tensor(0.3385)\n",
      "tensor(0.3385)\n",
      "tensor(0.3384)\n",
      "tensor(0.3383)\n",
      "tensor(0.3383)\n",
      "tensor(0.3382)\n",
      "tensor(0.3381)\n",
      "tensor(0.3380)\n",
      "tensor(0.3380)\n",
      "tensor(0.3379)\n",
      "tensor(0.3378)\n",
      "tensor(0.3378)\n",
      "tensor(0.3377)\n",
      "tensor(0.3376)\n",
      "tensor(0.3375)\n",
      "tensor(0.3375)\n",
      "tensor(0.3374)\n",
      "tensor(0.3373)\n",
      "tensor(0.3372)\n",
      "tensor(0.3372)\n",
      "tensor(0.3371)\n",
      "tensor(0.3370)\n",
      "tensor(0.3370)\n",
      "tensor(0.3369)\n",
      "tensor(0.3368)\n",
      "tensor(0.3367)\n",
      "tensor(0.3367)\n",
      "tensor(0.3366)\n",
      "tensor(0.3365)\n",
      "tensor(0.3365)\n",
      "tensor(0.3364)\n",
      "tensor(0.3363)\n",
      "tensor(0.3362)\n",
      "tensor(0.3362)\n",
      "tensor(0.3361)\n",
      "tensor(0.3360)\n",
      "tensor(0.3360)\n",
      "tensor(0.3359)\n",
      "tensor(0.3358)\n",
      "tensor(0.3357)\n",
      "tensor(0.3357)\n",
      "tensor(0.3356)\n",
      "tensor(0.3355)\n",
      "tensor(0.3355)\n",
      "tensor(0.3354)\n",
      "tensor(0.3353)\n",
      "tensor(0.3352)\n",
      "tensor(0.3352)\n",
      "tensor(0.3351)\n",
      "tensor(0.3350)\n",
      "tensor(0.3350)\n",
      "tensor(0.3349)\n",
      "tensor(0.3348)\n",
      "tensor(0.3347)\n",
      "tensor(0.3347)\n",
      "tensor(0.3346)\n",
      "tensor(0.3345)\n",
      "tensor(0.3345)\n",
      "tensor(0.3344)\n",
      "tensor(0.3343)\n",
      "tensor(0.3342)\n",
      "tensor(0.3342)\n",
      "tensor(0.3341)\n",
      "tensor(0.3340)\n",
      "tensor(0.3340)\n",
      "tensor(0.3339)\n",
      "tensor(0.3338)\n",
      "tensor(0.3337)\n",
      "tensor(0.3337)\n",
      "tensor(0.3336)\n",
      "tensor(0.3335)\n",
      "tensor(0.3335)\n",
      "tensor(0.3334)\n",
      "tensor(0.3333)\n",
      "tensor(0.3332)\n",
      "tensor(0.3332)\n",
      "tensor(0.3331)\n",
      "tensor(0.3330)\n",
      "tensor(0.3330)\n",
      "tensor(0.3329)\n",
      "tensor(0.3328)\n",
      "tensor(0.3328)\n",
      "tensor(0.3327)\n",
      "tensor(0.3326)\n",
      "tensor(0.3325)\n",
      "tensor(0.3325)\n",
      "tensor(0.3324)\n",
      "tensor(0.3323)\n",
      "tensor(0.3323)\n",
      "tensor(0.3322)\n",
      "tensor(0.3321)\n",
      "tensor(0.3320)\n",
      "tensor(0.3320)\n",
      "tensor(0.3319)\n",
      "tensor(0.3318)\n",
      "tensor(0.3318)\n",
      "tensor(0.3317)\n",
      "tensor(0.3316)\n",
      "tensor(0.3316)\n",
      "tensor(0.3315)\n",
      "tensor(0.3314)\n",
      "tensor(0.3313)\n",
      "tensor(0.3313)\n",
      "tensor(0.3312)\n",
      "tensor(0.3311)\n",
      "tensor(0.3311)\n",
      "tensor(0.3310)\n",
      "tensor(0.3309)\n",
      "tensor(0.3309)\n",
      "tensor(0.3308)\n",
      "tensor(0.3307)\n",
      "tensor(0.3306)\n",
      "tensor(0.3306)\n",
      "tensor(0.3305)\n",
      "tensor(0.3304)\n",
      "tensor(0.3304)\n",
      "tensor(0.3303)\n",
      "tensor(0.3302)\n",
      "tensor(0.3302)\n",
      "tensor(0.3301)\n",
      "tensor(0.3300)\n",
      "tensor(0.3299)\n",
      "tensor(0.3299)\n",
      "tensor(0.3298)\n",
      "tensor(0.3297)\n",
      "tensor(0.3297)\n",
      "tensor(0.3296)\n",
      "tensor(0.3295)\n",
      "tensor(0.3295)\n",
      "tensor(0.3294)\n",
      "tensor(0.3293)\n",
      "tensor(0.3293)\n",
      "tensor(0.3292)\n",
      "tensor(0.3291)\n",
      "tensor(0.3290)\n",
      "tensor(0.3290)\n",
      "tensor(0.3289)\n",
      "tensor(0.3288)\n",
      "tensor(0.3288)\n",
      "tensor(0.3287)\n",
      "tensor(0.3286)\n",
      "tensor(0.3286)\n",
      "tensor(0.3285)\n",
      "tensor(0.3284)\n",
      "tensor(0.3283)\n",
      "tensor(0.3283)\n",
      "tensor(0.3282)\n",
      "tensor(0.3281)\n",
      "tensor(0.3281)\n",
      "tensor(0.3280)\n",
      "tensor(0.3279)\n",
      "tensor(0.3279)\n",
      "tensor(0.3278)\n",
      "tensor(0.3277)\n",
      "tensor(0.3277)\n",
      "tensor(0.3276)\n",
      "tensor(0.3275)\n",
      "tensor(0.3275)\n",
      "tensor(0.3274)\n",
      "tensor(0.3273)\n",
      "tensor(0.3272)\n",
      "tensor(0.3272)\n",
      "tensor(0.3271)\n",
      "tensor(0.3270)\n",
      "tensor(0.3270)\n",
      "tensor(0.3269)\n",
      "tensor(0.3268)\n",
      "tensor(0.3268)\n",
      "tensor(0.3267)\n",
      "tensor(0.3266)\n",
      "tensor(0.3266)\n",
      "tensor(0.3265)\n",
      "tensor(0.3264)\n",
      "tensor(0.3264)\n",
      "tensor(0.3263)\n",
      "tensor(0.3262)\n",
      "tensor(0.3261)\n",
      "tensor(0.3261)\n",
      "tensor(0.3260)\n",
      "tensor(0.3259)\n",
      "tensor(0.3259)\n",
      "tensor(0.3258)\n",
      "tensor(0.3257)\n",
      "tensor(0.3257)\n",
      "tensor(0.3256)\n",
      "tensor(0.3255)\n",
      "tensor(0.3255)\n",
      "tensor(0.3254)\n",
      "tensor(0.3253)\n",
      "tensor(0.3253)\n",
      "tensor(0.3252)\n",
      "tensor(0.3251)\n",
      "tensor(0.3251)\n",
      "tensor(0.3250)\n",
      "tensor(0.3249)\n",
      "tensor(0.3248)\n",
      "tensor(0.3248)\n",
      "tensor(0.3247)\n",
      "tensor(0.3246)\n",
      "tensor(0.3246)\n",
      "tensor(0.3245)\n",
      "tensor(0.3244)\n",
      "tensor(0.3244)\n",
      "tensor(0.3243)\n",
      "tensor(0.3242)\n",
      "tensor(0.3242)\n",
      "tensor(0.3241)\n",
      "tensor(0.3240)\n",
      "tensor(0.3240)\n",
      "tensor(0.3239)\n",
      "tensor(0.3238)\n",
      "tensor(0.3238)\n",
      "tensor(0.3237)\n",
      "tensor(0.3236)\n",
      "tensor(0.3236)\n",
      "tensor(0.3235)\n",
      "tensor(0.3234)\n",
      "tensor(0.3234)\n",
      "tensor(0.3233)\n",
      "tensor(0.3232)\n",
      "tensor(0.3231)\n",
      "tensor(0.3231)\n",
      "tensor(0.3230)\n",
      "tensor(0.3229)\n",
      "tensor(0.3229)\n",
      "tensor(0.3228)\n",
      "tensor(0.3227)\n",
      "tensor(0.3227)\n",
      "tensor(0.3226)\n",
      "tensor(0.3225)\n",
      "tensor(0.3225)\n",
      "tensor(0.3224)\n",
      "tensor(0.3223)\n",
      "tensor(0.3223)\n",
      "tensor(0.3222)\n",
      "tensor(0.3221)\n",
      "tensor(0.3221)\n",
      "tensor(0.3220)\n",
      "tensor(0.3219)\n",
      "tensor(0.3219)\n",
      "tensor(0.3218)\n",
      "tensor(0.3217)\n",
      "tensor(0.3217)\n",
      "tensor(0.3216)\n",
      "tensor(0.3215)\n",
      "tensor(0.3215)\n",
      "tensor(0.3214)\n",
      "tensor(0.3213)\n",
      "tensor(0.3213)\n",
      "tensor(0.3212)\n",
      "tensor(0.3211)\n",
      "tensor(0.3211)\n",
      "tensor(0.3210)\n",
      "tensor(0.3209)\n",
      "tensor(0.3209)\n",
      "tensor(0.3208)\n",
      "tensor(0.3207)\n",
      "tensor(0.3207)\n",
      "tensor(0.3206)\n",
      "tensor(0.3205)\n",
      "tensor(0.3205)\n",
      "tensor(0.3204)\n",
      "tensor(0.3203)\n",
      "tensor(0.3203)\n",
      "tensor(0.3202)\n",
      "tensor(0.3201)\n",
      "tensor(0.3201)\n",
      "tensor(0.3200)\n",
      "tensor(0.3199)\n",
      "tensor(0.3199)\n",
      "tensor(0.3198)\n",
      "tensor(0.3197)\n",
      "tensor(0.3197)\n",
      "tensor(0.3196)\n",
      "tensor(0.3195)\n",
      "tensor(0.3195)\n",
      "tensor(0.3194)\n",
      "tensor(0.3193)\n",
      "tensor(0.3193)\n",
      "tensor(0.3192)\n",
      "tensor(0.3191)\n",
      "tensor(0.3191)\n",
      "tensor(0.3190)\n",
      "tensor(0.3189)\n",
      "tensor(0.3189)\n",
      "tensor(0.3188)\n",
      "tensor(0.3187)\n",
      "tensor(0.3187)\n",
      "tensor(0.3186)\n",
      "tensor(0.3185)\n",
      "tensor(0.3185)\n",
      "tensor(0.3184)\n",
      "tensor(0.3183)\n",
      "tensor(0.3183)\n",
      "tensor(0.3182)\n",
      "tensor(0.3181)\n",
      "tensor(0.3181)\n",
      "tensor(0.3180)\n",
      "tensor(0.3179)\n",
      "tensor(0.3179)\n",
      "tensor(0.3178)\n",
      "tensor(0.3177)\n",
      "tensor(0.3177)\n",
      "tensor(0.3176)\n",
      "tensor(0.3175)\n",
      "tensor(0.3175)\n",
      "tensor(0.3174)\n",
      "tensor(0.3173)\n",
      "tensor(0.3173)\n",
      "tensor(0.3172)\n",
      "tensor(0.3171)\n",
      "tensor(0.3171)\n",
      "tensor(0.3170)\n",
      "tensor(0.3169)\n",
      "tensor(0.3169)\n",
      "tensor(0.3168)\n",
      "tensor(0.3168)\n",
      "tensor(0.3167)\n",
      "tensor(0.3166)\n",
      "tensor(0.3166)\n",
      "tensor(0.3165)\n",
      "tensor(0.3164)\n",
      "tensor(0.3164)\n",
      "tensor(0.3163)\n",
      "tensor(0.3162)\n",
      "tensor(0.3162)\n",
      "tensor(0.3161)\n",
      "tensor(0.3160)\n",
      "tensor(0.3160)\n",
      "tensor(0.3159)\n",
      "tensor(0.3158)\n",
      "tensor(0.3158)\n",
      "tensor(0.3157)\n",
      "tensor(0.3156)\n",
      "tensor(0.3156)\n",
      "tensor(0.3155)\n",
      "tensor(0.3154)\n",
      "tensor(0.3154)\n",
      "tensor(0.3153)\n",
      "tensor(0.3152)\n",
      "tensor(0.3152)\n",
      "tensor(0.3151)\n",
      "tensor(0.3150)\n",
      "tensor(0.3150)\n",
      "tensor(0.3149)\n",
      "tensor(0.3149)\n",
      "tensor(0.3148)\n",
      "tensor(0.3147)\n",
      "tensor(0.3147)\n",
      "tensor(0.3146)\n",
      "tensor(0.3145)\n",
      "tensor(0.3145)\n",
      "tensor(0.3144)\n",
      "tensor(0.3143)\n",
      "tensor(0.3143)\n",
      "tensor(0.3142)\n",
      "tensor(0.3141)\n",
      "tensor(0.3141)\n",
      "tensor(0.3140)\n",
      "tensor(0.3139)\n",
      "tensor(0.3139)\n",
      "tensor(0.3138)\n",
      "tensor(0.3138)\n",
      "tensor(0.3137)\n",
      "tensor(0.3136)\n",
      "tensor(0.3136)\n",
      "tensor(0.3135)\n",
      "tensor(0.3134)\n",
      "tensor(0.3134)\n",
      "tensor(0.3133)\n",
      "tensor(0.3132)\n",
      "tensor(0.3132)\n",
      "tensor(0.3131)\n",
      "tensor(0.3130)\n",
      "tensor(0.3130)\n",
      "tensor(0.3129)\n",
      "tensor(0.3128)\n",
      "tensor(0.3128)\n",
      "tensor(0.3127)\n",
      "tensor(0.3127)\n",
      "tensor(0.3126)\n",
      "tensor(0.3125)\n",
      "tensor(0.3125)\n",
      "tensor(0.3124)\n",
      "tensor(0.3123)\n",
      "tensor(0.3123)\n",
      "tensor(0.3122)\n",
      "tensor(0.3121)\n",
      "tensor(0.3121)\n",
      "tensor(0.3120)\n",
      "tensor(0.3119)\n",
      "tensor(0.3119)\n",
      "tensor(0.3118)\n",
      "tensor(0.3118)\n",
      "tensor(0.3117)\n",
      "tensor(0.3116)\n",
      "tensor(0.3116)\n",
      "tensor(0.3115)\n",
      "tensor(0.3114)\n",
      "tensor(0.3114)\n",
      "tensor(0.3113)\n",
      "tensor(0.3112)\n",
      "tensor(0.3112)\n",
      "tensor(0.3111)\n",
      "tensor(0.3110)\n",
      "tensor(0.3110)\n",
      "tensor(0.3109)\n",
      "tensor(0.3109)\n",
      "tensor(0.3108)\n",
      "tensor(0.3107)\n",
      "tensor(0.3107)\n",
      "tensor(0.3106)\n",
      "tensor(0.3105)\n",
      "tensor(0.3105)\n",
      "tensor(0.3104)\n",
      "tensor(0.3103)\n",
      "tensor(0.3103)\n",
      "tensor(0.3102)\n",
      "tensor(0.3102)\n",
      "tensor(0.3101)\n",
      "tensor(0.3100)\n",
      "tensor(0.3100)\n",
      "tensor(0.3099)\n",
      "tensor(0.3098)\n",
      "tensor(0.3098)\n",
      "tensor(0.3097)\n",
      "tensor(0.3096)\n",
      "tensor(0.3096)\n",
      "tensor(0.3095)\n",
      "tensor(0.3095)\n",
      "tensor(0.3094)\n",
      "tensor(0.3093)\n",
      "tensor(0.3093)\n",
      "tensor(0.3092)\n",
      "tensor(0.3091)\n",
      "tensor(0.3091)\n",
      "tensor(0.3090)\n",
      "tensor(0.3089)\n",
      "tensor(0.3089)\n",
      "tensor(0.3088)\n",
      "tensor(0.3088)\n",
      "tensor(0.3087)\n",
      "tensor(0.3086)\n",
      "tensor(0.3086)\n",
      "tensor(0.3085)\n",
      "tensor(0.3084)\n",
      "tensor(0.3084)\n",
      "tensor(0.3083)\n",
      "tensor(0.3083)\n",
      "tensor(0.3082)\n",
      "tensor(0.3081)\n",
      "tensor(0.3081)\n",
      "tensor(0.3080)\n",
      "tensor(0.3079)\n",
      "tensor(0.3079)\n",
      "tensor(0.3078)\n",
      "tensor(0.3077)\n",
      "tensor(0.3077)\n",
      "tensor(0.3076)\n",
      "tensor(0.3076)\n",
      "tensor(0.3075)\n",
      "tensor(0.3074)\n",
      "tensor(0.3074)\n",
      "tensor(0.3073)\n",
      "tensor(0.3072)\n",
      "tensor(0.3072)\n",
      "tensor(0.3071)\n",
      "tensor(0.3071)\n",
      "tensor(0.3070)\n",
      "tensor(0.3069)\n",
      "tensor(0.3069)\n",
      "tensor(0.3068)\n",
      "tensor(0.3067)\n",
      "tensor(0.3067)\n",
      "tensor(0.3066)\n",
      "tensor(0.3066)\n",
      "tensor(0.3065)\n",
      "tensor(0.3064)\n",
      "tensor(0.3064)\n",
      "tensor(0.3063)\n",
      "tensor(0.3062)\n",
      "tensor(0.3062)\n",
      "tensor(0.3061)\n",
      "tensor(0.3061)\n",
      "tensor(0.3060)\n",
      "tensor(0.3059)\n",
      "tensor(0.3059)\n",
      "tensor(0.3058)\n",
      "tensor(0.3057)\n",
      "tensor(0.3057)\n",
      "tensor(0.3056)\n",
      "tensor(0.3056)\n",
      "tensor(0.3055)\n",
      "tensor(0.3054)\n",
      "tensor(0.3054)\n",
      "tensor(0.3053)\n",
      "tensor(0.3052)\n",
      "tensor(0.3052)\n",
      "tensor(0.3051)\n",
      "tensor(0.3051)\n",
      "tensor(0.3050)\n",
      "tensor(0.3049)\n",
      "tensor(0.3049)\n",
      "tensor(0.3048)\n",
      "tensor(0.3047)\n",
      "tensor(0.3047)\n",
      "tensor(0.3046)\n",
      "tensor(0.3046)\n",
      "tensor(0.3045)\n",
      "tensor(0.3044)\n",
      "tensor(0.3044)\n",
      "tensor(0.3043)\n",
      "tensor(0.3042)\n",
      "tensor(0.3042)\n",
      "tensor(0.3041)\n",
      "tensor(0.3041)\n",
      "tensor(0.3040)\n",
      "tensor(0.3039)\n",
      "tensor(0.3039)\n",
      "tensor(0.3038)\n",
      "tensor(0.3038)\n",
      "tensor(0.3037)\n",
      "tensor(0.3036)\n",
      "tensor(0.3036)\n",
      "tensor(0.3035)\n",
      "tensor(0.3034)\n",
      "tensor(0.3034)\n",
      "tensor(0.3033)\n",
      "tensor(0.3033)\n",
      "tensor(0.3032)\n",
      "tensor(0.3031)\n",
      "tensor(0.3031)\n",
      "tensor(0.3030)\n",
      "tensor(0.3030)\n",
      "tensor(0.3029)\n",
      "tensor(0.3028)\n",
      "tensor(0.3028)\n",
      "tensor(0.3027)\n",
      "tensor(0.3026)\n",
      "tensor(0.3026)\n",
      "tensor(0.3025)\n",
      "tensor(0.3025)\n",
      "tensor(0.3024)\n",
      "tensor(0.3023)\n",
      "tensor(0.3023)\n",
      "tensor(0.3022)\n",
      "tensor(0.3022)\n",
      "tensor(0.3021)\n",
      "tensor(0.3020)\n",
      "tensor(0.3020)\n",
      "tensor(0.3019)\n",
      "tensor(0.3018)\n",
      "tensor(0.3018)\n",
      "tensor(0.3017)\n",
      "tensor(0.3017)\n",
      "tensor(0.3016)\n",
      "tensor(0.3015)\n",
      "tensor(0.3015)\n",
      "tensor(0.3014)\n",
      "tensor(0.3014)\n",
      "tensor(0.3013)\n",
      "tensor(0.3012)\n",
      "tensor(0.3012)\n",
      "tensor(0.3011)\n",
      "tensor(0.3011)\n",
      "tensor(0.3010)\n",
      "tensor(0.3009)\n",
      "tensor(0.3009)\n",
      "tensor(0.3008)\n",
      "tensor(0.3007)\n",
      "tensor(0.3007)\n",
      "tensor(0.3006)\n",
      "tensor(0.3006)\n",
      "tensor(0.3005)\n",
      "tensor(0.3004)\n",
      "tensor(0.3004)\n",
      "tensor(0.3003)\n",
      "tensor(0.3003)\n",
      "tensor(0.3002)\n",
      "tensor(0.3001)\n",
      "tensor(0.3001)\n",
      "tensor(0.3000)\n",
      "tensor(0.3000)\n",
      "tensor(0.2999)\n",
      "tensor(0.2998)\n",
      "tensor(0.2998)\n",
      "tensor(0.2997)\n",
      "tensor(0.2997)\n",
      "tensor(0.2996)\n",
      "tensor(0.2995)\n",
      "tensor(0.2995)\n",
      "tensor(0.2994)\n",
      "tensor(0.2994)\n",
      "tensor(0.2993)\n",
      "tensor(0.2992)\n",
      "tensor(0.2992)\n",
      "tensor(0.2991)\n",
      "tensor(0.2991)\n",
      "tensor(0.2990)\n",
      "tensor(0.2989)\n",
      "tensor(0.2989)\n",
      "tensor(0.2988)\n",
      "tensor(0.2987)\n",
      "tensor(0.2987)\n",
      "tensor(0.2986)\n",
      "tensor(0.2986)\n",
      "tensor(0.2985)\n",
      "tensor(0.2984)\n",
      "tensor(0.2984)\n",
      "tensor(0.2983)\n",
      "tensor(0.2983)\n",
      "tensor(0.2982)\n",
      "tensor(0.2981)\n",
      "tensor(0.2981)\n",
      "tensor(0.2980)\n",
      "tensor(0.2980)\n",
      "tensor(0.2979)\n",
      "tensor(0.2978)\n",
      "tensor(0.2978)\n",
      "tensor(0.2977)\n",
      "tensor(0.2977)\n",
      "tensor(0.2976)\n",
      "tensor(0.2975)\n",
      "tensor(0.2975)\n",
      "tensor(0.2974)\n",
      "tensor(0.2974)\n",
      "tensor(0.2973)\n",
      "tensor(0.2972)\n",
      "tensor(0.2972)\n",
      "tensor(0.2971)\n",
      "tensor(0.2971)\n",
      "tensor(0.2970)\n",
      "tensor(0.2969)\n",
      "tensor(0.2969)\n",
      "tensor(0.2968)\n",
      "tensor(0.2968)\n",
      "tensor(0.2967)\n",
      "tensor(0.2967)\n",
      "tensor(0.2966)\n",
      "tensor(0.2965)\n",
      "tensor(0.2965)\n",
      "tensor(0.2964)\n",
      "tensor(0.2964)\n",
      "tensor(0.2963)\n",
      "tensor(0.2962)\n",
      "tensor(0.2962)\n",
      "tensor(0.2961)\n",
      "tensor(0.2961)\n",
      "tensor(0.2960)\n",
      "tensor(0.2959)\n",
      "tensor(0.2959)\n",
      "tensor(0.2958)\n",
      "tensor(0.2958)\n",
      "tensor(0.2957)\n",
      "tensor(0.2956)\n",
      "tensor(0.2956)\n",
      "tensor(0.2955)\n",
      "tensor(0.2955)\n",
      "tensor(0.2954)\n",
      "tensor(0.2953)\n",
      "tensor(0.2953)\n",
      "tensor(0.2952)\n",
      "tensor(0.2952)\n",
      "tensor(0.2951)\n",
      "tensor(0.2950)\n",
      "tensor(0.2950)\n",
      "tensor(0.2949)\n",
      "tensor(0.2949)\n",
      "tensor(0.2948)\n",
      "tensor(0.2947)\n",
      "tensor(0.2947)\n",
      "tensor(0.2946)\n",
      "tensor(0.2946)\n",
      "tensor(0.2945)\n",
      "tensor(0.2945)\n",
      "tensor(0.2944)\n",
      "tensor(0.2943)\n",
      "tensor(0.2943)\n",
      "tensor(0.2942)\n",
      "tensor(0.2942)\n",
      "tensor(0.2941)\n",
      "tensor(0.2940)\n",
      "tensor(0.2940)\n",
      "tensor(0.2939)\n",
      "tensor(0.2939)\n",
      "tensor(0.2938)\n",
      "tensor(0.2937)\n",
      "tensor(0.2937)\n",
      "tensor(0.2936)\n",
      "tensor(0.2936)\n",
      "tensor(0.2935)\n",
      "tensor(0.2935)\n",
      "tensor(0.2934)\n",
      "tensor(0.2933)\n",
      "tensor(0.2933)\n",
      "tensor(0.2932)\n",
      "tensor(0.2932)\n",
      "tensor(0.2931)\n",
      "tensor(0.2930)\n",
      "tensor(0.2930)\n",
      "tensor(0.2929)\n",
      "tensor(0.2929)\n",
      "tensor(0.2928)\n",
      "tensor(0.2928)\n",
      "tensor(0.2927)\n",
      "tensor(0.2926)\n",
      "tensor(0.2926)\n",
      "tensor(0.2925)\n",
      "tensor(0.2925)\n",
      "tensor(0.2924)\n",
      "tensor(0.2923)\n",
      "tensor(0.2923)\n",
      "tensor(0.2922)\n",
      "tensor(0.2922)\n",
      "tensor(0.2921)\n",
      "tensor(0.2920)\n",
      "tensor(0.2920)\n",
      "tensor(0.2919)\n",
      "tensor(0.2919)\n",
      "tensor(0.2918)\n",
      "tensor(0.2918)\n",
      "tensor(0.2917)\n",
      "tensor(0.2916)\n",
      "tensor(0.2916)\n",
      "tensor(0.2915)\n",
      "tensor(0.2915)\n",
      "tensor(0.2914)\n",
      "tensor(0.2914)\n",
      "tensor(0.2913)\n",
      "tensor(0.2912)\n",
      "tensor(0.2912)\n",
      "tensor(0.2911)\n",
      "tensor(0.2911)\n",
      "tensor(0.2910)\n",
      "tensor(0.2909)\n",
      "tensor(0.2909)\n",
      "tensor(0.2908)\n",
      "tensor(0.2908)\n",
      "tensor(0.2907)\n",
      "tensor(0.2907)\n",
      "tensor(0.2906)\n",
      "tensor(0.2905)\n",
      "tensor(0.2905)\n",
      "tensor(0.2904)\n",
      "tensor(0.2904)\n",
      "tensor(0.2903)\n",
      "tensor(0.2902)\n",
      "tensor(0.2902)\n",
      "tensor(0.2901)\n",
      "tensor(0.2901)\n",
      "tensor(0.2900)\n",
      "tensor(0.2900)\n",
      "tensor(0.2899)\n",
      "tensor(0.2898)\n",
      "tensor(0.2898)\n",
      "tensor(0.2897)\n",
      "tensor(0.2897)\n",
      "tensor(0.2896)\n",
      "tensor(0.2896)\n",
      "tensor(0.2895)\n",
      "tensor(0.2894)\n",
      "tensor(0.2894)\n",
      "tensor(0.2893)\n",
      "tensor(0.2893)\n",
      "tensor(0.2892)\n",
      "tensor(0.2892)\n",
      "tensor(0.2891)\n",
      "tensor(0.2890)\n",
      "tensor(0.2890)\n",
      "tensor(0.2889)\n",
      "tensor(0.2889)\n",
      "tensor(0.2888)\n",
      "tensor(0.2888)\n",
      "tensor(0.2887)\n",
      "tensor(0.2886)\n",
      "tensor(0.2886)\n",
      "tensor(0.2885)\n",
      "tensor(0.2885)\n",
      "tensor(0.2884)\n",
      "tensor(0.2884)\n",
      "tensor(0.2883)\n",
      "tensor(0.2882)\n",
      "tensor(0.2882)\n",
      "tensor(0.2881)\n",
      "tensor(0.2881)\n",
      "tensor(0.2880)\n",
      "tensor(0.2880)\n",
      "tensor(0.2879)\n",
      "tensor(0.2878)\n",
      "tensor(0.2878)\n",
      "tensor(0.2877)\n",
      "tensor(0.2877)\n",
      "tensor(0.2876)\n",
      "tensor(0.2876)\n",
      "tensor(0.2875)\n",
      "tensor(0.2874)\n",
      "tensor(0.2874)\n",
      "tensor(0.2873)\n",
      "tensor(0.2873)\n",
      "tensor(0.2872)\n",
      "tensor(0.2872)\n",
      "tensor(0.2871)\n",
      "tensor(0.2870)\n",
      "tensor(0.2870)\n",
      "tensor(0.2869)\n",
      "tensor(0.2869)\n",
      "tensor(0.2868)\n",
      "tensor(0.2868)\n",
      "tensor(0.2867)\n",
      "tensor(0.2866)\n",
      "tensor(0.2866)\n",
      "tensor(0.2865)\n",
      "tensor(0.2865)\n",
      "tensor(0.2864)\n",
      "tensor(0.2864)\n",
      "tensor(0.2863)\n",
      "tensor(0.2862)\n",
      "tensor(0.2862)\n",
      "tensor(0.2861)\n",
      "tensor(0.2861)\n",
      "tensor(0.2860)\n",
      "tensor(0.2860)\n",
      "tensor(0.2859)\n",
      "tensor(0.2859)\n",
      "tensor(0.2858)\n",
      "tensor(0.2857)\n",
      "tensor(0.2857)\n",
      "tensor(0.2856)\n",
      "tensor(0.2856)\n",
      "tensor(0.2855)\n",
      "tensor(0.2855)\n",
      "tensor(0.2854)\n",
      "tensor(0.2853)\n",
      "tensor(0.2853)\n",
      "tensor(0.2852)\n",
      "tensor(0.2852)\n",
      "tensor(0.2851)\n",
      "tensor(0.2851)\n",
      "tensor(0.2850)\n",
      "tensor(0.2849)\n",
      "tensor(0.2849)\n",
      "tensor(0.2848)\n",
      "tensor(0.2848)\n",
      "tensor(0.2847)\n",
      "tensor(0.2847)\n",
      "tensor(0.2846)\n",
      "tensor(0.2846)\n",
      "tensor(0.2845)\n",
      "tensor(0.2844)\n",
      "tensor(0.2844)\n",
      "tensor(0.2843)\n",
      "tensor(0.2843)\n",
      "tensor(0.2842)\n",
      "tensor(0.2842)\n",
      "tensor(0.2841)\n",
      "tensor(0.2841)\n",
      "tensor(0.2840)\n",
      "tensor(0.2839)\n",
      "tensor(0.2839)\n",
      "tensor(0.2838)\n",
      "tensor(0.2838)\n",
      "tensor(0.2837)\n",
      "tensor(0.2837)\n",
      "tensor(0.2836)\n",
      "tensor(0.2835)\n",
      "tensor(0.2835)\n",
      "tensor(0.2834)\n",
      "tensor(0.2834)\n",
      "tensor(0.2833)\n",
      "tensor(0.2833)\n",
      "tensor(0.2832)\n",
      "tensor(0.2832)\n",
      "tensor(0.2831)\n",
      "tensor(0.2830)\n",
      "tensor(0.2830)\n",
      "tensor(0.2829)\n",
      "tensor(0.2829)\n",
      "tensor(0.2828)\n",
      "tensor(0.2828)\n",
      "tensor(0.2827)\n",
      "tensor(0.2827)\n",
      "tensor(0.2826)\n",
      "tensor(0.2825)\n",
      "tensor(0.2825)\n",
      "tensor(0.2824)\n",
      "tensor(0.2824)\n",
      "tensor(0.2823)\n",
      "tensor(0.2823)\n",
      "tensor(0.2822)\n",
      "tensor(0.2822)\n",
      "tensor(0.2821)\n",
      "tensor(0.2820)\n",
      "tensor(0.2820)\n",
      "tensor(0.2819)\n",
      "tensor(0.2819)\n",
      "tensor(0.2818)\n",
      "tensor(0.2818)\n",
      "tensor(0.2817)\n",
      "tensor(0.2817)\n",
      "tensor(0.2816)\n",
      "tensor(0.2815)\n",
      "tensor(0.2815)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "for i in range(num_epochs):\n",
    "    optimizer.zero_grad() # 1\n",
    "    y_pred = sigmoid(linear_model(x_train)) # 2\n",
    "    loss = bce_loss(y_pred, y_train) #3\n",
    "    loss.backward() # 4\n",
    "    optimizer.step() # 5\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(loss.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 클래스를 통한 회귀 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        # 부모클래스인 nn.Module의 생성자를 먼저 호출한다.\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear_layer = nn.Linear(1,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 순전파 함수: 입력값 x를 생성자에 정의된 레이어에 넣어서 값을 예측한다.\n",
    "        return self.linear_layer(x)\n",
    "\n",
    "\n",
    "class MultiRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiRegression, self).__init__()\n",
    "        self.multi_layer = nn.Linear(3,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.multi_layer(x)\n",
    "\n",
    "# 퀴즈: 로지스틱 회귀 모델을 클래스를 통해 구현하세요.\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression,self).__init__()\n",
    "        self.linear_layer = nn.Linear(2,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_layer(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, optimizer, loss_func, x_train, y_train, epochs=1):\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        optimizer.zero_grad() # 1\n",
    "        y_pred = model(x_train) # 2\n",
    "        loss = loss_func(y_pred, y_train) #3\n",
    "        loss.backward() # 4\n",
    "        optimizer.step() # 5\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'epoch : {i+1}/{epochs}, loss : {loss.data}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 2], [1, 2], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
      "tensor([[0., 2.],\n",
      "        [1., 2.],\n",
      "        [3., 1.],\n",
      "        [4., 3.],\n",
      "        [5., 3.],\n",
      "        [6., 2.]]) torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "# 칠판에 그리기\n",
    "x_data = [[0, 2], [1, 2], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)\n",
    "print(x_data)\n",
    "print(x_train,x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = nn.Linear(2,1)\n",
    "sigmoid = nn.Sigmoid()\n",
    "logistic_model = LogisticRegression()\n",
    "optimizer = optim.Adam(logistic_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/1000, loss : 0.7762554287910461\n",
      "epoch : 101/1000, loss : 0.6940462589263916\n",
      "epoch : 201/1000, loss : 0.6428194642066956\n",
      "epoch : 301/1000, loss : 0.5999968647956848\n",
      "epoch : 401/1000, loss : 0.5635276436805725\n",
      "epoch : 501/1000, loss : 0.532690703868866\n",
      "epoch : 601/1000, loss : 0.5066280364990234\n",
      "epoch : 701/1000, loss : 0.48447513580322266\n",
      "epoch : 801/1000, loss : 0.4654538333415985\n",
      "epoch : 901/1000, loss : 0.44890615344047546\n"
     ]
    }
   ],
   "source": [
    "fit(logistic_model, optimizer, bce_loss, x_train, y_train, 1000)\n",
    "# class LogisticRegression(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(LogisticRegression,self).__init__()\n",
    "#         self.linear_layer = nn.Linear(2,1)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.linear_layer(x)\n",
    "#         x = self.sigmoid(x)\n",
    "#         return x\n",
    "# LogisticRegression()(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiabetesDataset(Dataset):\n",
    "    def __init__(self, xy_dataset):\n",
    "        # 커스텀 데이터셋 클래스의 생성자를 정의\n",
    "        # 데이터를 불러와서 torch.tensor로 할당 및 전처리한다.\n",
    "        self.x_data = torch.from_numpy(xy_dataset[:,0:-1])\n",
    "        self.y_data = torch.from_numpy(xy_dataset[:,[-1]])\n",
    "        print(f'X shape : {self.x_data.shape} | Y shape : {self.y_data.shape}')\n",
    "\n",
    "    # 매직 메소드 : 함수 이름 앞, 뒤로 underbar 2개를 붙인 매소드\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_data[idx], self.y_data[idx]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.loadtxt('./data/diabetes.csv.gz', delimiter=',', dtype=np.float32)\n",
    "dataset = DiabetesDataset(xy)\n",
    "data_loader = DataLoader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "생성자\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.layer1 = nn.Linear(8,6)\n",
    "        self.layer2 = nn.Linear(6,4)\n",
    "        self.layer3 = nn.Linear(4,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        z1 = self.sigmoid(self.layer1(x))\n",
    "        z2 = self.sigmoid(self.layer2(z1))\n",
    "        z3 = self.sigmoid(self.layer3(z2))\n",
    "        y_pred = sigmoid(z3)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 퀴즈 (Normal)  \n",
    "위 세 가지 모델을 구현했으면 또 반복문을 통해 학습시켜야 합니다.  \n",
    "이는 귀찮은 과정이니 함수 형태로 만들어서 코드의 반복을 줄여봅시다.  \n",
    "텐서플로우에서 사용했던 fit 함수를 직접 만들어봅시다.  \n",
    "fit() 함수는 model, optimizer, loss_func, x_train, y_train, epochs를 입력으로 받습니다.  \n",
    "위에서 수행한 반복문을 함수형태로 만들어서 세 가지 회귀모델에 적용할 것입니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax 회귀  \n",
    "https://wikidocs.net/60575"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "a39879ae9ccb927afc0cf2ce944a9e89e93d1a75059893b60dcfa8646ca8faf1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
