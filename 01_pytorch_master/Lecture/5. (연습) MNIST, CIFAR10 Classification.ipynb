{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "from torchvision import datasets, transforms"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "batch_size = 32\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1a9d6fa79a9743fd8ff7666de4c9d0c3"
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b2723edaebf4476afab2507d0e2f6e8"
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "25e7535ad7fe4a87ad2caf0d2cf0ab99"
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4f169e27c8114184896d390b197d6a24"
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "print(len(train_dataset))\n",
    "print(train_dataset.classes)\n",
    "print(train_dataset.data.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "60000\n",
      "['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n",
      "torch.Size([60000, 28, 28])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 퀴즈 (Easy)  \n",
    "Multi Layered Perceptrion 을 통해 MNIST 데이터셋을 분류하려면  \n",
    "1) 모델의 첫 번째 레이어의 입력크기  \n",
    "2) 모델의 출력크기는 어떻게 되어야할까요?  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 퀴즈 (Easy)  \n",
    "간단한 MLP 모델을 구현해봅시다. \n",
    "1) 은닉층은 총 4개로 (784, 512, 256, 128) 개의 뉴런이 존재합니다. \n",
    "  \n",
    "  \n",
    "2) 활성화함수는 relu를 사용합니다.  \n",
    "\n",
    "3) forward 함수에 x를 처음 입력받을 때 기존에 배운 flatten() 또는 reshape() 또는 view()를 활용해서 일차원 벡터로 변환하세요"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "class MLP(nn.Module): # Multi Layers perceptron\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.l1 = nn.Linear(784, 512)\n",
    "        self.l2 = nn.Linear(512, 256)\n",
    "        self.l3 = nn.Linear(256, 128)\n",
    "        self.l4 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Flatten the data (n, 1, 28, 28)-> (n, 784)\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        y_pred = F.softmax(self.l4(x))\n",
    "\n",
    "        return y_pred\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "이제 train, test 함수를 작성해보겠습니다.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## train, test 함수 작성"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "model = MLP().to(device)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(total_params)\n",
    "\n",
    "def train(epoch, model, loss_func, train_loader, optimizer):\n",
    "    model.train()\n",
    "    for batch_index, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = loss_func(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_index % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch+1} | Batch Status: {batch_index*len(x)}/{len(train_loader.dataset)} \\\n",
    "            ({100. * batch_index * batch_size / len(train_loader.dataset):.0f}% | Loss: {loss.item():.6f}')\n",
    "\n",
    "def test(model, loss_func, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct_count = 0\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        test_loss += loss_func(y_pred, y).item()\n",
    "        pred = y_pred.data.max(1, keepdim=True)[1]\n",
    "        # torch.eq : Computes element-wise equality. return counts value\n",
    "        correct_count += pred.eq(y.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'=======================\\n Test set: Average loss: {test_loss:.4f}, Accuracy: {correct_count/len(test_loader.dataset):.3}')\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cpu\n",
      "567434\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## train 및 test 실행  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "test(model, ce_loss, test_loader)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_2897/3758966121.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  y_pred = F.softmax(self.l4(x))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=======================\n",
      " Test set: Average loss: 0.0721, Accuracy: 0.103\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "for epoch in range(0,3):\n",
    "    train(epoch, model, ce_loss, train_loader, optimizer)\n",
    "    test(model, ce_loss, test_loader)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_2897/3758966121.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  y_pred = F.softmax(self.l4(x))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Epoch: 1 | Batch Status: 0/60000             (0% | Loss: 2.303427\n",
      "Train Epoch: 1 | Batch Status: 3200/60000             (5% | Loss: 1.671340\n",
      "Train Epoch: 1 | Batch Status: 6400/60000             (11% | Loss: 1.644079\n",
      "Train Epoch: 1 | Batch Status: 9600/60000             (16% | Loss: 1.620211\n",
      "Train Epoch: 1 | Batch Status: 12800/60000             (21% | Loss: 1.584070\n",
      "Train Epoch: 1 | Batch Status: 16000/60000             (27% | Loss: 1.651800\n",
      "Train Epoch: 1 | Batch Status: 19200/60000             (32% | Loss: 1.616539\n",
      "Train Epoch: 1 | Batch Status: 22400/60000             (37% | Loss: 1.613604\n",
      "Train Epoch: 1 | Batch Status: 25600/60000             (43% | Loss: 1.560839\n",
      "Train Epoch: 1 | Batch Status: 28800/60000             (48% | Loss: 1.493674\n",
      "Train Epoch: 1 | Batch Status: 32000/60000             (53% | Loss: 1.491674\n",
      "Train Epoch: 1 | Batch Status: 35200/60000             (59% | Loss: 1.492555\n",
      "Train Epoch: 1 | Batch Status: 38400/60000             (64% | Loss: 1.578172\n",
      "Train Epoch: 1 | Batch Status: 41600/60000             (69% | Loss: 1.537326\n",
      "Train Epoch: 1 | Batch Status: 44800/60000             (75% | Loss: 1.555021\n",
      "Train Epoch: 1 | Batch Status: 48000/60000             (80% | Loss: 1.552270\n",
      "Train Epoch: 1 | Batch Status: 51200/60000             (85% | Loss: 1.601815\n",
      "Train Epoch: 1 | Batch Status: 54400/60000             (91% | Loss: 1.540709\n",
      "Train Epoch: 1 | Batch Status: 57600/60000             (96% | Loss: 1.553898\n",
      "=======================\n",
      " Test set: Average loss: 0.0477, Accuracy: 0.935\n",
      "Train Epoch: 2 | Batch Status: 0/60000             (0% | Loss: 1.461852\n",
      "Train Epoch: 2 | Batch Status: 3200/60000             (5% | Loss: 1.531988\n",
      "Train Epoch: 2 | Batch Status: 6400/60000             (11% | Loss: 1.524089\n",
      "Train Epoch: 2 | Batch Status: 9600/60000             (16% | Loss: 1.557915\n",
      "Train Epoch: 2 | Batch Status: 12800/60000             (21% | Loss: 1.526947\n",
      "Train Epoch: 2 | Batch Status: 16000/60000             (27% | Loss: 1.491405\n",
      "Train Epoch: 2 | Batch Status: 19200/60000             (32% | Loss: 1.560951\n",
      "Train Epoch: 2 | Batch Status: 22400/60000             (37% | Loss: 1.492321\n",
      "Train Epoch: 2 | Batch Status: 25600/60000             (43% | Loss: 1.523039\n",
      "Train Epoch: 2 | Batch Status: 28800/60000             (48% | Loss: 1.499581\n",
      "Train Epoch: 2 | Batch Status: 32000/60000             (53% | Loss: 1.523340\n",
      "Train Epoch: 2 | Batch Status: 35200/60000             (59% | Loss: 1.546215\n",
      "Train Epoch: 2 | Batch Status: 38400/60000             (64% | Loss: 1.522292\n",
      "Train Epoch: 2 | Batch Status: 41600/60000             (69% | Loss: 1.554901\n",
      "Train Epoch: 2 | Batch Status: 44800/60000             (75% | Loss: 1.539419\n",
      "Train Epoch: 2 | Batch Status: 48000/60000             (80% | Loss: 1.491442\n",
      "Train Epoch: 2 | Batch Status: 51200/60000             (85% | Loss: 1.563812\n",
      "Train Epoch: 2 | Batch Status: 54400/60000             (91% | Loss: 1.461150\n",
      "Train Epoch: 2 | Batch Status: 57600/60000             (96% | Loss: 1.461163\n",
      "=======================\n",
      " Test set: Average loss: 0.0476, Accuracy: 0.939\n",
      "Train Epoch: 3 | Batch Status: 0/60000             (0% | Loss: 1.552733\n",
      "Train Epoch: 3 | Batch Status: 3200/60000             (5% | Loss: 1.630230\n",
      "Train Epoch: 3 | Batch Status: 6400/60000             (11% | Loss: 1.554911\n",
      "Train Epoch: 3 | Batch Status: 9600/60000             (16% | Loss: 1.554891\n",
      "Train Epoch: 3 | Batch Status: 12800/60000             (21% | Loss: 1.500341\n",
      "Train Epoch: 3 | Batch Status: 16000/60000             (27% | Loss: 1.461150\n",
      "Train Epoch: 3 | Batch Status: 19200/60000             (32% | Loss: 1.481392\n",
      "Train Epoch: 3 | Batch Status: 22400/60000             (37% | Loss: 1.534593\n",
      "Train Epoch: 3 | Batch Status: 25600/60000             (43% | Loss: 1.488469\n",
      "Train Epoch: 3 | Batch Status: 28800/60000             (48% | Loss: 1.471044\n",
      "Train Epoch: 3 | Batch Status: 32000/60000             (53% | Loss: 1.492400\n",
      "Train Epoch: 3 | Batch Status: 35200/60000             (59% | Loss: 1.461150\n",
      "Train Epoch: 3 | Batch Status: 38400/60000             (64% | Loss: 1.489333\n",
      "Train Epoch: 3 | Batch Status: 41600/60000             (69% | Loss: 1.532517\n",
      "Train Epoch: 3 | Batch Status: 44800/60000             (75% | Loss: 1.492617\n",
      "Train Epoch: 3 | Batch Status: 48000/60000             (80% | Loss: 1.522639\n",
      "Train Epoch: 3 | Batch Status: 51200/60000             (85% | Loss: 1.555330\n",
      "Train Epoch: 3 | Batch Status: 54400/60000             (91% | Loss: 1.461150\n",
      "Train Epoch: 3 | Batch Status: 57600/60000             (96% | Loss: 1.516375\n",
      "=======================\n",
      " Test set: Average loss: 0.0472, Accuracy: 0.952\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MNIST Classification with CNN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, C, W, H, K, S):\n",
    "        super(CNN, self).__init__()\n",
    "        # nn.Module에는 이미 Conv 레이어가 구현되어 있습니다.\n",
    "        # 마찬가지로 배치정규화 레이어도 구현되어 있습니다.\n",
    "        self.conv1 = nn.Conv2d(C, 32, kernel_size=K, stride=S)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=K, stride=S)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=K, stride=S)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size=K, stride=S):\n",
    "            print((size - (kernel_size - 1) - 1) // stride + 1)\n",
    "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        \n",
    "        convw = conv2d_size_out(W, K, S)\n",
    "        convw = conv2d_size_out(convw, K, S)        \n",
    "        convw = conv2d_size_out(convw, K, S)       \n",
    "        \n",
    "        self.linear_input_size = convw * convw * 128\n",
    "        self.fc = nn.Linear(self.linear_input_size, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1) # (batch_size, flaaten_size)\n",
    "        x = F.relu(self.fc(x))\n",
    "        return F.log_softmax(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "cnn = CNN(C=1, W=28, H=28, K=3, S=2) \n",
    "cnn = cnn.to(device)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=0.001)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "13\n",
      "6\n",
      "2\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "total_params = sum(p.numel() for p in cnn.parameters())\n",
    "print(total_params)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "98250\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "for epoch in range(0,1):\n",
    "    train(epoch, cnn, ce_loss, train_loader, optimizer)\n",
    "    test(cnn, ce_loss, test_loader)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_2897/807479046.py:30: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Epoch: 1 | Batch Status: 0/60000             (0% | Loss: 0.489507\n",
      "Train Epoch: 1 | Batch Status: 3200/60000             (5% | Loss: 0.649534\n",
      "Train Epoch: 1 | Batch Status: 6400/60000             (11% | Loss: 0.874837\n",
      "Train Epoch: 1 | Batch Status: 9600/60000             (16% | Loss: 0.520191\n",
      "Train Epoch: 1 | Batch Status: 12800/60000             (21% | Loss: 0.876566\n",
      "Train Epoch: 1 | Batch Status: 16000/60000             (27% | Loss: 0.782363\n",
      "Train Epoch: 1 | Batch Status: 19200/60000             (32% | Loss: 0.781238\n",
      "Train Epoch: 1 | Batch Status: 22400/60000             (37% | Loss: 0.822598\n",
      "Train Epoch: 1 | Batch Status: 25600/60000             (43% | Loss: 0.719959\n",
      "Train Epoch: 1 | Batch Status: 28800/60000             (48% | Loss: 0.371852\n",
      "Train Epoch: 1 | Batch Status: 32000/60000             (53% | Loss: 0.507295\n",
      "Train Epoch: 1 | Batch Status: 35200/60000             (59% | Loss: 0.893156\n",
      "Train Epoch: 1 | Batch Status: 38400/60000             (64% | Loss: 0.382471\n",
      "Train Epoch: 1 | Batch Status: 41600/60000             (69% | Loss: 0.510258\n",
      "Train Epoch: 1 | Batch Status: 44800/60000             (75% | Loss: 0.584016\n",
      "Train Epoch: 1 | Batch Status: 48000/60000             (80% | Loss: 1.018066\n",
      "Train Epoch: 1 | Batch Status: 51200/60000             (85% | Loss: 0.640205\n",
      "Train Epoch: 1 | Batch Status: 54400/60000             (91% | Loss: 0.662193\n",
      "Train Epoch: 1 | Batch Status: 57600/60000             (96% | Loss: 0.581567\n",
      "=======================\n",
      " Test set: Average loss: 0.0220, Accuracy: 0.703\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "source": [],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "98250\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_layers, block, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=16, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # feature map size = 32x32x16\n",
    "        self.layers_2n = self.get_layers(block, 16, 16, stride=1)\n",
    "        # feature map size = 16x16x32\n",
    "        self.layers_4n = self.get_layers(block, 16, 32, stride=2)\n",
    "        # feature map size = 8x8x64\n",
    "        self.layers_6n = self.get_layers(block, 32, 64, stride=2)\n",
    "\n",
    "        # output layers\n",
    "        # self.avg_pool = nn.AvgPool2d(8, stride=1)\n",
    "        self.fc_out = nn.Linear(49 * 64, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out',\n",
    "                                        nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def get_layers(self, block, in_channels, out_channels, stride):\n",
    "        if stride == 2:\n",
    "            down_sample = True\n",
    "        else:\n",
    "            down_sample = False\n",
    "\n",
    "        layers_list = nn.ModuleList(\n",
    "            [block(in_channels, out_channels, stride, down_sample)])\n",
    "\n",
    "        for _ in range(self.num_layers - 1):\n",
    "            layers_list.append(block(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layers_2n(x)\n",
    "        x = self.layers_4n(x)\n",
    "        x = self.layers_6n(x)\n",
    "\n",
    "        #x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, down_sample=False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.stride = stride\n",
    "\n",
    "        if down_sample:\n",
    "            self.down_sample = IdentityPadding(in_channels, out_channels, stride)\n",
    "        else:\n",
    "            self.down_sample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.down_sample is not None:\n",
    "            shortcut = self.down_sam    ple(x)\n",
    "\n",
    "        out += shortcut\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class IdentityPadding(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(IdentityPadding, self).__init__()\n",
    "\n",
    "        self.pooling = nn.MaxPool2d(1, stride=stride)\n",
    "        self.add_channels = out_channels - in_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))\n",
    "        out = self.pooling(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet_model():\n",
    "    block = ResidualBlock\n",
    "    model = ResNet(1, 3, block)\n",
    "    return model\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "resnet = resnet_model()\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "total_params = sum(p.numel() for p in resnet.parameters())\n",
    "print(total_params)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "300154\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "for epoch in range(0,1):\n",
    "    train(epoch, resnet, ce_loss, train_loader, optimizer)\n",
    "    test(resnet, ce_loss, test_loader)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Epoch: 1 | Batch Status: 0/60000             (0% | Loss: 2.873370\n",
      "Train Epoch: 1 | Batch Status: 3200/60000             (5% | Loss: 2.935059\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2897/350268288.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mce_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mce_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2897/1065950393.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, model, loss_func, train_loader, optimizer)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Working/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Working/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Assignment  \n",
    "\n",
    "1) cifar10 데이터셋, 데이터로더를 할당하는 코드를 실행해서 데이터셋을 다운받으세요.  \n",
    "2) 데이터셋의 채널과 모양을 확인해보세요. 그리고 채널과 모양은 CNN을 구현할 때 반영하세요.  \n",
    "3) conv layer가 5개인 CNN 모델을 구현하세요 모델의 총 파라미터 수는 약 30만개여야 합니다. 배치정규화를 사용하세요.    \n",
    "4) 구현한 CNN 모델을 학습시키세요. learning_rate=0.001, optimizer=Adam, Epochs=100  \n",
    "5) 구현되어 있는 Resnet 모델을 불러와서 학습시켜보세요. 옵티마이저와 하이퍼파라미터는 기존과 동일합니다.  \n",
    "6) 두 모델 중 어느 것이 더 좋은가요?  \n",
    "7) 필요하다면 구글링을 적극적으로 하세요!!   "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('Working': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "026aceb1435085fcef523649cdfc9385a4a55dbc5c65435142607853821fa50a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}