{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# yaml tutorial"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "이전에 argparse_tutorial을 진행하면서 어떻게 인자들을 파싱하는지를 배웠습니다.  \n",
    "하지만 이렇게 입력하는 과정도 어쩐지 귀찮다는 생각이 듭니다.  \n",
    "이 과정을 자동화하기 위해서 설정 파일을 만들어 놓고 이를 불러오는 식으로 사용하게 됩니다.  \n",
    "보통 설정파일의 형식은 yaml 또는 json을 많이 사용합니다. yaml의 경우 하이퍼파라미터, json의 경우 데이터의 라벨을 주로 기록합니다.  \n",
    "예를 들면 사용하는 형식은 다음과 같습니다. \n",
    "\n",
    "python run_cnn.py --model_path 'C:/Users/Aiffel/cnn/configs/cnn.yaml'"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## config 파일 생성  \n",
    "그럼 이제 직접 yaml 파일을 만들어 보고 불러와보겠습니다.  \n",
    "1) Practice 폴더 내에 configs 폴더를 만드세요.   \n",
    "2) configs 폴더 내에 cnn.yaml 파일을 만드세요.  \n",
    "3) argparse에서 사용했던 하이퍼파라미터 정보들을 그대로 기록하고 저장하세요.  \n",
    "4) yaml 파일 내에서는 키: 값 의 쌍으로 값을 저장합니다.  \n",
    "ex)  \n",
    "leaning_rate: 0.001"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## yaml 파일 로드"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import yaml\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# with 구문으로 파일을 불러옵니다.\n",
    "\n",
    "with open('configs/cnn.yaml') as f:\n",
    "    config = yaml.load(f,Loader=yaml.FullLoader)\n",
    "    print(type(config))\n",
    "\n",
    "def print_config(**kwargs):\n",
    "    for key, value in kwargs.items():\n",
    "        print(key,value)\n",
    "\n",
    "print_config(**config)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'dict'>\n",
      "batch_size 16\n",
      "learning_rate 0.001\n",
      "epochs 3\n",
      "kernel_size 2\n",
      "stride 2\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Quiz (Hard)   \n",
    "이제 argparse와 yaml을 합쳐서 편하게 정보를 불러오는 실행파일을 만들어봅시다.  \n",
    "\n",
    "1) run_cnn2 라는 파이썬 파일을 만드세요.  \n",
    "2) argparse를 이용해 config_path라는 argument를 추가해줍니다. 인자의 타입은 스트링입니다.  \n",
    "3) 위에서 구현한 yaml 파일을 로드하는 코드에서 config_path를 활용하여 하이퍼파라미터 정보를 불러오세요\n",
    "4) 모든 하이퍼파라미터들을 출력해서 정상적으로 불러왔는지 확인하세요.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "%%writefile run_cnn2.py\n",
    "\n",
    "'''\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "epochs = 5\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "'''\n",
    "import os \n",
    "import argparse\n",
    "\n",
    "# parser 정의\n",
    "parser = argparse.ArgumentParser(description='Argparse Tutorial')\n",
    "# add_argument()를 통해 argument의 이름, 타입, 기본 값, 도움말을 정의할수 있다.\n",
    "# parser.add_argument('-b','--batch_size', type=int, default=16, help=\"Score of korean\")\n",
    "# parser.add_argument('-l', '--learning_rate', type=float, default=0.1,help=\"Score of mathematcis\")\n",
    "# parser.add_argument('-e', '--epochs', type=int, default=1, help = \"Score of english\")\n",
    "# parser.add_argument('-k', '--kernel_size', type=int, default=3, help = \"Score of english\")\n",
    "# parser.add_argument('-s', '--stride', type=int, default=2, help = \"Score of english\")\n",
    "parser.add_argument('-c', '--config_path', type=str, default='configs/cnn.yaml', help = \"Score of english\")\n",
    "\n",
    "\n",
    "\n",
    "# add_argment()함수를 호출하면 parser인스턴스 내부에 해당 이름을 가지는 멤버 변수를 생성\n",
    "# parse_arg()를 통해 프로그램 실행시 parser가 실행되도록 합니다.\n",
    "args = parser.parse_args()\n",
    "\n",
    "# subject_info = {'korean': args.n}\n",
    "def run_cnn(args):\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    import torch.optim as optim\n",
    "    import numpy as np\n",
    "\n",
    "    from torch.utils.data import DataLoader, Dataset \n",
    "    from torchvision import datasets, transforms\n",
    "\n",
    "    import yaml\n",
    "    # with 구문으로 파일을 불러옵니다.\n",
    "\n",
    "    with open(args.config_path) as f:\n",
    "        config = yaml.load(f,Loader=yaml.FullLoader)\n",
    "        print(type(config))\n",
    "\n",
    "    # def print_config(**kwargs):\n",
    "    #     for key, value in kwargs.items():\n",
    "    #         if key == 'batch_size':\n",
    "    #             batch_size = value\n",
    "    #         if key == 'learning_rate':\n",
    "    #             learning_rate = value    \n",
    "    #         if key == 'epochs':\n",
    "    #             epochs = value\n",
    "    #         if key == 'kernel_size':\n",
    "    #             kernel_size = value\n",
    "    #         if key == 'stride':\n",
    "    #             stride = value\n",
    "            \n",
    "\n",
    "    # Hyperparameters\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    epochs = config['epochs']\n",
    "    kernel_size = config['kernel_size']\n",
    "    stride = config['stride']\n",
    "\n",
    "    # print_config(**config)\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    train_dataset = datasets.MNIST(root='./mnist_data/', train=True, download=True, transform=transforms.ToTensor())\n",
    "    valid_dataset = datasets.MNIST(root='./mnist_data/', train=False, download=True, transform=transforms.ToTensor())\n",
    "    test_dataset = datasets.MNIST(root='./mnist_data/', train=False, download=True, transform=transforms.ToTensor())\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    vaild_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    shape = train_dataset[0][0].shape\n",
    "    print(shape)\n",
    "    C = shape[0]\n",
    "    W = shape[1]\n",
    "    H = shape[2]\n",
    "    print(C, W, H)\n",
    "\n",
    "    def train(epoch, model, loss_func, train_loader, optimizer):\n",
    "        model.train()\n",
    "        for batch_index, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            loss = loss_func(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_index % 100 == 0:\n",
    "                print(f'Train Epoch: {epoch+1} | Batch Status: {batch_index*len(x)}/{len(train_loader.dataset)} \\\n",
    "                ({100. * batch_index * batch_size / len(train_loader.dataset):.0f}% | Loss: {loss.item():.6f}')\n",
    "\n",
    "    def test(model, loss_func, test_loader):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct_count = 0\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "            test_loss += loss_func(y_pred, y).item()\n",
    "            pred = y_pred.data.max(1, keepdim=True)[1]\n",
    "            # torch.eq : Computes element-wise equality. return counts value\n",
    "            correct_count += pred.eq(y.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print(f'=======================\\n Test set: Average loss: {test_loss:.4f}, Accuracy: {correct_count/len(test_loader.dataset):.3}')\n",
    "\n",
    "    class CNN(nn.Module):\n",
    "        def __init__(self, C, W, H, K, S): # 채널, 너비, 높이, 커널 사이즈, 스트라이드\n",
    "            super(CNN, self).__init__()\n",
    "            # nn.Module에는 이미 conv 레이어가 구현되어 있다. \n",
    "            # 배치정규화도 구현되어있고 다 구현되어있습니다. \n",
    "            self.conv1 = nn.Conv2d(C, 32, kernel_size=K, stride=S)\n",
    "            self.bn1 = nn.BatchNorm2d(32)\n",
    "            self.conv2 = nn.Conv2d(32, 64, kernel_size=K, stride=S)\n",
    "            self.bn2 = nn.BatchNorm2d(64)\n",
    "            self.conv3 = nn.Conv2d(64, 128, kernel_size=K, stride=S)\n",
    "            self.bn3 = nn.BatchNorm2d(128)\n",
    "            \n",
    "            def conv2d_size_out(size, kernel_size=K, stride=S):\n",
    "                print((size - (kernel_size - 1) - 1) // stride + 1)\n",
    "                return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "            \n",
    "            convw = conv2d_size_out(W, K, S)\n",
    "            convw = conv2d_size_out(convw, K, S)\n",
    "            convw = conv2d_size_out(convw, K, S)\n",
    "            \n",
    "            self.linear_input_size = convw * convw * 128\n",
    "            self.fc = nn.Linear(self.linear_input_size, 10)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.bn1(self.conv1(x)))\n",
    "            x = F.relu(self.bn2(self.conv2(x)))\n",
    "            x = F.relu(self.bn3(self.conv3(x)))\n",
    "            x = x.view(x.size(0), -1) # (batch_size, flatten_size)\n",
    "            x = F.relu(self.fc(x))\n",
    "            return F.log_softmax(x)\n",
    "\n",
    "    cnn = CNN(C=C, W=W, H=H, K=kernel_size, S=stride) \n",
    "    cnn = cnn.to(device)\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(cnn.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train(epoch, cnn, ce_loss, train_loader, optimizer)\n",
    "\n",
    "    test(cnn, ce_loss, test_loader)\n",
    "run_cnn(args)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting run_cnn2.py\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "!python3 run_cnn2.py"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'dict'>\n",
      "torch.Size([1, 28, 28])\n",
      "1 28 28\n",
      "14\n",
      "7\n",
      "3\n",
      "run_cnn2.py:146: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n",
      "Train Epoch: 1 | Batch Status: 0/60000                 (0% | Loss: 2.449309\n",
      "Train Epoch: 1 | Batch Status: 1600/60000                 (3% | Loss: 1.806441\n",
      "Train Epoch: 1 | Batch Status: 3200/60000                 (5% | Loss: 1.225666\n",
      "Train Epoch: 1 | Batch Status: 4800/60000                 (8% | Loss: 1.189337\n",
      "Train Epoch: 1 | Batch Status: 6400/60000                 (11% | Loss: 1.456013\n",
      "Train Epoch: 1 | Batch Status: 8000/60000                 (13% | Loss: 1.347386\n",
      "Train Epoch: 1 | Batch Status: 9600/60000                 (16% | Loss: 1.831234\n",
      "Train Epoch: 1 | Batch Status: 11200/60000                 (19% | Loss: 1.019140\n",
      "Train Epoch: 1 | Batch Status: 12800/60000                 (21% | Loss: 1.041910\n",
      "Train Epoch: 1 | Batch Status: 14400/60000                 (24% | Loss: 1.630642\n",
      "Train Epoch: 1 | Batch Status: 16000/60000                 (27% | Loss: 1.584034\n",
      "Train Epoch: 1 | Batch Status: 17600/60000                 (29% | Loss: 2.079732\n",
      "Train Epoch: 1 | Batch Status: 19200/60000                 (32% | Loss: 1.934943\n",
      "Train Epoch: 1 | Batch Status: 20800/60000                 (35% | Loss: 1.303652\n",
      "Train Epoch: 1 | Batch Status: 22400/60000                 (37% | Loss: 1.021168\n",
      "Train Epoch: 1 | Batch Status: 24000/60000                 (40% | Loss: 0.726678\n",
      "Train Epoch: 1 | Batch Status: 25600/60000                 (43% | Loss: 1.739560\n",
      "Train Epoch: 1 | Batch Status: 27200/60000                 (45% | Loss: 1.343242\n",
      "Train Epoch: 1 | Batch Status: 28800/60000                 (48% | Loss: 0.913935\n",
      "Train Epoch: 1 | Batch Status: 30400/60000                 (51% | Loss: 1.299025\n",
      "Train Epoch: 1 | Batch Status: 32000/60000                 (53% | Loss: 1.300590\n",
      "Train Epoch: 1 | Batch Status: 33600/60000                 (56% | Loss: 1.311984\n",
      "Train Epoch: 1 | Batch Status: 35200/60000                 (59% | Loss: 1.454224\n",
      "Train Epoch: 1 | Batch Status: 36800/60000                 (61% | Loss: 1.744395\n",
      "Train Epoch: 1 | Batch Status: 38400/60000                 (64% | Loss: 1.738984\n",
      "Train Epoch: 1 | Batch Status: 40000/60000                 (67% | Loss: 1.444243\n",
      "Train Epoch: 1 | Batch Status: 41600/60000                 (69% | Loss: 0.866534\n",
      "Train Epoch: 1 | Batch Status: 43200/60000                 (72% | Loss: 1.207504\n",
      "Train Epoch: 1 | Batch Status: 44800/60000                 (75% | Loss: 0.599371\n",
      "Train Epoch: 1 | Batch Status: 46400/60000                 (77% | Loss: 1.024094\n",
      "Train Epoch: 1 | Batch Status: 48000/60000                 (80% | Loss: 1.355066\n",
      "Train Epoch: 1 | Batch Status: 49600/60000                 (83% | Loss: 0.828346\n",
      "Train Epoch: 1 | Batch Status: 51200/60000                 (85% | Loss: 1.666845\n",
      "Train Epoch: 1 | Batch Status: 52800/60000                 (88% | Loss: 1.375467\n",
      "Train Epoch: 1 | Batch Status: 54400/60000                 (91% | Loss: 1.586247\n",
      "Train Epoch: 1 | Batch Status: 56000/60000                 (93% | Loss: 0.893456\n",
      "Train Epoch: 1 | Batch Status: 57600/60000                 (96% | Loss: 1.153095\n",
      "Train Epoch: 1 | Batch Status: 59200/60000                 (99% | Loss: 1.727319\n",
      "Train Epoch: 2 | Batch Status: 0/60000                 (0% | Loss: 1.152108\n",
      "Train Epoch: 2 | Batch Status: 1600/60000                 (3% | Loss: 1.365181\n",
      "Train Epoch: 2 | Batch Status: 3200/60000                 (5% | Loss: 1.727025\n",
      "Train Epoch: 2 | Batch Status: 4800/60000                 (8% | Loss: 2.074143\n",
      "Train Epoch: 2 | Batch Status: 6400/60000                 (11% | Loss: 1.151557\n",
      "Train Epoch: 2 | Batch Status: 8000/60000                 (13% | Loss: 1.727065\n",
      "Train Epoch: 2 | Batch Status: 9600/60000                 (16% | Loss: 1.179210\n",
      "Train Epoch: 2 | Batch Status: 11200/60000                 (19% | Loss: 1.008714\n",
      "Train Epoch: 2 | Batch Status: 12800/60000                 (21% | Loss: 1.441446\n",
      "Train Epoch: 2 | Batch Status: 14400/60000                 (24% | Loss: 1.840760\n",
      "Train Epoch: 2 | Batch Status: 16000/60000                 (27% | Loss: 1.600552\n",
      "Train Epoch: 2 | Batch Status: 17600/60000                 (29% | Loss: 1.426624\n",
      "Train Epoch: 2 | Batch Status: 19200/60000                 (32% | Loss: 1.296169\n",
      "Train Epoch: 2 | Batch Status: 20800/60000                 (35% | Loss: 1.441881\n",
      "Train Epoch: 2 | Batch Status: 22400/60000                 (37% | Loss: 1.992464\n",
      "Train Epoch: 2 | Batch Status: 24000/60000                 (40% | Loss: 1.288771\n",
      "Train Epoch: 2 | Batch Status: 25600/60000                 (43% | Loss: 1.261102\n",
      "Train Epoch: 2 | Batch Status: 27200/60000                 (45% | Loss: 0.879860\n",
      "Train Epoch: 2 | Batch Status: 28800/60000                 (48% | Loss: 1.305714\n",
      "Train Epoch: 2 | Batch Status: 30400/60000                 (51% | Loss: 1.561774\n",
      "Train Epoch: 2 | Batch Status: 32000/60000                 (53% | Loss: 1.731987\n",
      "Train Epoch: 2 | Batch Status: 33600/60000                 (56% | Loss: 1.341589\n",
      "Train Epoch: 2 | Batch Status: 35200/60000                 (59% | Loss: 1.295728\n",
      "Train Epoch: 2 | Batch Status: 36800/60000                 (61% | Loss: 1.297529\n",
      "Train Epoch: 2 | Batch Status: 38400/60000                 (64% | Loss: 1.309245\n",
      "Train Epoch: 2 | Batch Status: 40000/60000                 (67% | Loss: 1.585046\n",
      "Train Epoch: 2 | Batch Status: 41600/60000                 (69% | Loss: 1.671121\n",
      "Train Epoch: 2 | Batch Status: 43200/60000                 (72% | Loss: 1.614081\n",
      "Train Epoch: 2 | Batch Status: 44800/60000                 (75% | Loss: 1.296173\n",
      "Train Epoch: 2 | Batch Status: 46400/60000                 (77% | Loss: 2.019681\n",
      "Train Epoch: 2 | Batch Status: 48000/60000                 (80% | Loss: 1.687544\n",
      "Train Epoch: 2 | Batch Status: 49600/60000                 (83% | Loss: 1.872198\n",
      "Train Epoch: 2 | Batch Status: 51200/60000                 (85% | Loss: 1.454792\n",
      "Train Epoch: 2 | Batch Status: 52800/60000                 (88% | Loss: 1.163331\n",
      "Train Epoch: 2 | Batch Status: 54400/60000                 (91% | Loss: 1.015859\n",
      "Train Epoch: 2 | Batch Status: 56000/60000                 (93% | Loss: 1.876405\n",
      "Train Epoch: 2 | Batch Status: 57600/60000                 (96% | Loss: 1.600984\n",
      "Train Epoch: 2 | Batch Status: 59200/60000                 (99% | Loss: 0.720168\n",
      "Train Epoch: 3 | Batch Status: 0/60000                 (0% | Loss: 1.439271\n",
      "Train Epoch: 3 | Batch Status: 1600/60000                 (3% | Loss: 1.440056\n",
      "Train Epoch: 3 | Batch Status: 3200/60000                 (5% | Loss: 1.365562\n",
      "Train Epoch: 3 | Batch Status: 4800/60000                 (8% | Loss: 0.891292\n",
      "Train Epoch: 3 | Batch Status: 6400/60000                 (11% | Loss: 1.775130\n",
      "Train Epoch: 3 | Batch Status: 8000/60000                 (13% | Loss: 1.160261\n",
      "Train Epoch: 3 | Batch Status: 9600/60000                 (16% | Loss: 1.628545\n",
      "Train Epoch: 3 | Batch Status: 11200/60000                 (19% | Loss: 1.152439\n",
      "Train Epoch: 3 | Batch Status: 12800/60000                 (21% | Loss: 1.727036\n",
      "Train Epoch: 3 | Batch Status: 14400/60000                 (24% | Loss: 1.152098\n",
      "Train Epoch: 3 | Batch Status: 16000/60000                 (27% | Loss: 1.870858\n",
      "Train Epoch: 3 | Batch Status: 17600/60000                 (29% | Loss: 1.040242\n",
      "Train Epoch: 3 | Batch Status: 19200/60000                 (32% | Loss: 1.583235\n",
      "Train Epoch: 3 | Batch Status: 20800/60000                 (35% | Loss: 1.927694\n",
      "Train Epoch: 3 | Batch Status: 22400/60000                 (37% | Loss: 1.161584\n",
      "Train Epoch: 3 | Batch Status: 24000/60000                 (40% | Loss: 1.196992\n",
      "Train Epoch: 3 | Batch Status: 25600/60000                 (43% | Loss: 0.863888\n",
      "Train Epoch: 3 | Batch Status: 27200/60000                 (45% | Loss: 1.295398\n",
      "Train Epoch: 3 | Batch Status: 28800/60000                 (48% | Loss: 1.729136\n",
      "Train Epoch: 3 | Batch Status: 30400/60000                 (51% | Loss: 0.870903\n",
      "Train Epoch: 3 | Batch Status: 32000/60000                 (53% | Loss: 0.864214\n",
      "Train Epoch: 3 | Batch Status: 33600/60000                 (56% | Loss: 1.727133\n",
      "Train Epoch: 3 | Batch Status: 35200/60000                 (59% | Loss: 1.302471\n",
      "Train Epoch: 3 | Batch Status: 36800/60000                 (61% | Loss: 0.583120\n",
      "Train Epoch: 3 | Batch Status: 38400/60000                 (64% | Loss: 1.008715\n",
      "Train Epoch: 3 | Batch Status: 40000/60000                 (67% | Loss: 1.454178\n",
      "Train Epoch: 3 | Batch Status: 41600/60000                 (69% | Loss: 1.170821\n",
      "Train Epoch: 3 | Batch Status: 43200/60000                 (72% | Loss: 1.152106\n",
      "Train Epoch: 3 | Batch Status: 44800/60000                 (75% | Loss: 1.693178\n",
      "Train Epoch: 3 | Batch Status: 46400/60000                 (77% | Loss: 0.975563\n",
      "Train Epoch: 3 | Batch Status: 48000/60000                 (80% | Loss: 1.736776\n",
      "Train Epoch: 3 | Batch Status: 49600/60000                 (83% | Loss: 1.727094\n",
      "Train Epoch: 3 | Batch Status: 51200/60000                 (85% | Loss: 1.175444\n",
      "Train Epoch: 3 | Batch Status: 52800/60000                 (88% | Loss: 1.748793\n",
      "Train Epoch: 3 | Batch Status: 54400/60000                 (91% | Loss: 1.009848\n",
      "Train Epoch: 3 | Batch Status: 56000/60000                 (93% | Loss: 1.870946\n",
      "Train Epoch: 3 | Batch Status: 57600/60000                 (96% | Loss: 0.866756\n",
      "Train Epoch: 3 | Batch Status: 59200/60000                 (99% | Loss: 1.439168\n",
      "=======================\n",
      " Test set: Average loss: 0.0866, Accuracy: 0.506\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Json Tutorial  \n",
    "json 파일을 작성하고 불러오는 과정은 yaml 파일과 굉장히 유사합니다.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "JSON 파일의 예제는 아래와 같습니다. \n",
    "\n",
    "{\n",
    "\n",
    "   \"이름\": \"홍길동\",                          → 스트링\n",
    "\n",
    "   \"나이\": 25,                                      → 숫자 (정수)\n",
    "\n",
    "   \"특기\": [\"농구\", \"도술\"],              → list 표현 가능\n",
    "\n",
    "    \"가족관계\": {\"아버지\": \"홍판서\", \"어머니\": \"춘섬\"},  → array 표현 가능\n",
    "\n",
    "   \"결혼 여부\": true                          → Boolean \n",
    "\n",
    "}\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## json 파일 생성  \n",
    "마찬가지로 직접 json 파일을 만들어 보고 불러와보겠습니다.  \n",
    "1) Practice 폴더 내에 configs 폴더를 만드세요.  \n",
    "2) configs 폴더 내에 cnn.json 파일을 만드세요.  \n",
    "3) argparse에서 사용했던 하이퍼파라미터 정보들을 그대로 기록하고 저장하세요.  \n",
    "4) json 파일에서도 키: 값 의 쌍으로 값을 저장합니다.    \n",
    "5) yaml과의 차이점은 키 값을 쌍따옴표\"\"로 감싸야한다는 것과 {} 괄호 및 콤마, 를 사용하는 것입니다.  \n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('Working': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "026aceb1435085fcef523649cdfc9385a4a55dbc5c65435142607853821fa50a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}