{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## argparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 성적표 가중치 (한글, 수학, 영어) \n",
    "# 2. 고객 정보 저장 및 출력\n",
    "\n",
    "\n",
    "import os \n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os \n",
    "import argparse\n",
    "\n",
    "# parser 정의\n",
    "parser = argparse.ArgumentParser(description='Argparse Tutorial')\n",
    "# add_argument()를 통해 argument의 이름, 타입, 기본 값, 도움말을 정의할수 있다.\n",
    "parser.add_argument('--korean', type=int, default=0, help=\"Score of korean\")\n",
    "parser.add_argument('--mathematcis', type=int, default=0,help=\"Score of mathematcis\")\n",
    "parser.add_argument('-e', '--english', type=int, default=0, help = \"Score of english\")\n",
    "\n",
    "# add_argment()함수를 호출하면 parser인스턴스 내부에 해당 ㅣㅇ름을 가지는 멤버 변수를 생성\n",
    "# parse_arg()를 통해 프로그램 실행시 parser가 실행되도록 합니다.\n",
    "args = parser.parse_args()\n",
    "\n",
    "# subject_info = {'korean': args.n}\n",
    "def average(args):\n",
    "    total_score = 0\n",
    "    total_score += args.korean\n",
    "    total_score += args.mathematics\n",
    "    total_score += args.english\n",
    "    print(total_score/3)\n",
    "\n",
    "average(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: argparse_tutorial.py [-h] [--korean KOREAN] [--mathematcis MATHEMATCIS]\n",
      "                            [-e ENGLISH]\n",
      "\n",
      "Argparse Tutorial\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --korean KOREAN       Score of korean\n",
      "  --mathematcis MATHEMATCIS\n",
      "                        Score of mathematcis\n",
      "  -e ENGLISH, --english ENGLISH\n",
      "                        Score of english\n"
     ]
    }
   ],
   "source": [
    "!python argparse_tutorial.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.0\n"
     ]
    }
   ],
   "source": [
    "!python3 argparse_tutorial.py --korean 70 --mathematics 80 --english 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting user_info.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile user_info.py\n",
    "\n",
    "'''\n",
    "## Quiz (Normal)\n",
    "고객의 이름, 키, 발사이즈, 선호 브랜드 리스트를 입력받아서 저장하고 출력하는 프로그램을 작성하세요.\n",
    "\n",
    "argument의 종류는 --name, --height, --foot_size, --wish_list\n",
    "각각 type은 string, float, int, int\n",
    "각각 default는 '홍길동', 175.0, 270, [1234]\n",
    "'''\n",
    "import os \n",
    "import argparse\n",
    "\n",
    "# parser 정의\n",
    "parser = argparse.ArgumentParser(description='Argparse Tutorial')\n",
    "# add_argument()를 통해 argument의 이름, 타입, 기본 값, 도움말을 정의할수 있다.\n",
    "parser.add_argument('-n','--name', type=str, default='홍길동', help=\"Score of korean\")\n",
    "parser.add_argument('--height', type=float, default='175.0',help=\"Score of mathematcis\")\n",
    "parser.add_argument('-f', '--foot_size', type=int, default=270, help = \"Score of english\")\n",
    "parser.add_argument('-w', '--wish_list', type=list, default=[1,2,3,4], help = \"Score of english\")\n",
    "\n",
    "# add_argment()함수를 호출하면 parser인스턴스 내부에 해당 ㅣㅇ름을 가지는 멤버 변수를 생성\n",
    "# parse_arg()를 통해 프로그램 실행시 parser가 실행되도록 합니다.\n",
    "args = parser.parse_args()\n",
    "\n",
    "# subject_info = {'korean': args.n}\n",
    "def print_user_info(args):\n",
    "    print(f\"{args.name}의 키는{args.height}이고, 발 사이즈는 {args.wish_list}입니다.\")\n",
    "\n",
    "    \n",
    "\n",
    "print_user_info(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "홍길동의 키는175.0이고, 발 사이즈는 [1, 2, 3, 4]입니다.\n"
     ]
    }
   ],
   "source": [
    "!python3 user_info.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz (Easy)  \n",
    "딥러닝 모델의 하이퍼파라미터에는 무엇이 있었나요?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz (Easy)  \n",
    "argparser를 통해 아래의 파라미터를 입력 받을 수 있도록 파이썬 파일을 만들어보세요  \n",
    "파일의 이름은 run_cnn.py 로 만드세요.  \n",
    "위 블록의 내용을 활용해서 run_cnn의 상단에 하이퍼 파라미터들을 입력받는 argparser를 구현합니다.  \n",
    "입력받은 하이퍼파라미터들을 출력하세요. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz (Easy)\n",
    "아래 블록에 있는 모든 코드들을 복사해서 run_cnn 파이썬 파일에 붙여넣은 후,  \n",
    "입력 받은 하이퍼파라미터들을 적절한 위치에 할당하세요  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_cnn.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_cnn.py\n",
    "\n",
    "'''\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "epochs = 5\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "'''\n",
    "import os \n",
    "import argparse\n",
    "\n",
    "# parser 정의\n",
    "parser = argparse.ArgumentParser(description='Argparse Tutorial')\n",
    "# add_argument()를 통해 argument의 이름, 타입, 기본 값, 도움말을 정의할수 있다.\n",
    "parser.add_argument('-b','--batch_size', type=int, default=16, help=\"Score of korean\")\n",
    "parser.add_argument('-l', '--learning_rate', type=float, default=0.1,help=\"Score of mathematcis\")\n",
    "parser.add_argument('-e', '--epochs', type=int, default=1, help = \"Score of english\")\n",
    "parser.add_argument('-k', '--kernel_size', type=int, default=3, help = \"Score of english\")\n",
    "parser.add_argument('-s', '--stride', type=int, default=2, help = \"Score of english\")\n",
    "\n",
    "\n",
    "# add_argment()함수를 호출하면 parser인스턴스 내부에 해당 ㅣㅇ름을 가지는 멤버 변수를 생성\n",
    "# parse_arg()를 통해 프로그램 실행시 parser가 실행되도록 합니다.\n",
    "args = parser.parse_args()\n",
    "\n",
    "# subject_info = {'korean': args.n}\n",
    "def run_cnn(args):\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    import torch.optim as optim\n",
    "    import numpy as np\n",
    "\n",
    "    from torch.utils.data import DataLoader, Dataset \n",
    "    from torchvision import datasets, transforms\n",
    "\n",
    "    # Hyperparameters\n",
    "    batch_size = args.batch_size\n",
    "    learning_rate = args.learning_rate\n",
    "    epochs = args.epochs\n",
    "    kernel_size = args.kernel_size\n",
    "    stride = args.stride\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    train_dataset = datasets.MNIST(root='./mnist_data/', train=True, download=True, transform=transforms.ToTensor())\n",
    "    valid_dataset = datasets.MNIST(root='./mnist_data/', train=False, download=True, transform=transforms.ToTensor())\n",
    "    test_dataset = datasets.MNIST(root='./mnist_data/', train=False, download=True, transform=transforms.ToTensor())\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    vaild_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    shape = train_dataset[0][0].shape\n",
    "    print(shape)\n",
    "    C = shape[0]\n",
    "    W = shape[1]\n",
    "    H = shape[2]\n",
    "    print(C, W, H)\n",
    "\n",
    "    def train(epoch, model, loss_func, train_loader, optimizer):\n",
    "        model.train()\n",
    "        for batch_index, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            loss = loss_func(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_index % 100 == 0:\n",
    "                print(f'Train Epoch: {epoch+1} | Batch Status: {batch_index*len(x)}/{len(train_loader.dataset)} \\\n",
    "                ({100. * batch_index * batch_size / len(train_loader.dataset):.0f}% | Loss: {loss.item():.6f}')\n",
    "\n",
    "    def test(model, loss_func, test_loader):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct_count = 0\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "            test_loss += loss_func(y_pred, y).item()\n",
    "            pred = y_pred.data.max(1, keepdim=True)[1]\n",
    "            # torch.eq : Computes element-wise equality. return counts value\n",
    "            correct_count += pred.eq(y.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print(f'=======================\\n Test set: Average loss: {test_loss:.4f}, Accuracy: {correct_count/len(test_loader.dataset):.3}')\n",
    "\n",
    "    class CNN(nn.Module):\n",
    "        def __init__(self, C, W, H, K, S): # 채널, 너비, 높이, 커널 사이즈, 스트라이드\n",
    "            super(CNN, self).__init__()\n",
    "            # nn.Module에는 이미 conv 레이어가 구현되어 있다. \n",
    "            # 배치정규화도 구현되어있고 다 구현되어있습니다. \n",
    "            self.conv1 = nn.Conv2d(C, 32, kernel_size=K, stride=S)\n",
    "            self.bn1 = nn.BatchNorm2d(32)\n",
    "            self.conv2 = nn.Conv2d(32, 64, kernel_size=K, stride=S)\n",
    "            self.bn2 = nn.BatchNorm2d(64)\n",
    "            self.conv3 = nn.Conv2d(64, 128, kernel_size=K, stride=S)\n",
    "            self.bn3 = nn.BatchNorm2d(128)\n",
    "            \n",
    "            def conv2d_size_out(size, kernel_size=K, stride=S):\n",
    "                print((size - (kernel_size - 1) - 1) // stride + 1)\n",
    "                return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "            \n",
    "            convw = conv2d_size_out(W, K, S)\n",
    "            convw = conv2d_size_out(convw, K, S)\n",
    "            convw = conv2d_size_out(convw, K, S)\n",
    "            \n",
    "            self.linear_input_size = convw * convw * 128\n",
    "            self.fc = nn.Linear(self.linear_input_size, 10)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.bn1(self.conv1(x)))\n",
    "            x = F.relu(self.bn2(self.conv2(x)))\n",
    "            x = F.relu(self.bn3(self.conv3(x)))\n",
    "            x = x.view(x.size(0), -1) # (batch_size, flatten_size)\n",
    "            x = F.relu(self.fc(x))\n",
    "            return F.log_softmax(x)\n",
    "\n",
    "    cnn = CNN(C=C, W=W, H=H, K=kernel_size, S=stride) \n",
    "    cnn = cnn.to(device)\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(cnn.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train(epoch, cnn, ce_loss, train_loader, optimizer)\n",
    "\n",
    "    test(cnn, ce_loss, test_loader)\n",
    "run_cnn(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "1 28 28\n",
      "13\n",
      "6\n",
      "2\n",
      "run_cnn.py:121: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n",
      "Train Epoch: 1 | Batch Status: 0/60000                 (0% | Loss: 2.347040\n",
      "Train Epoch: 1 | Batch Status: 1600/60000                 (3% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 3200/60000                 (5% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 4800/60000                 (8% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 6400/60000                 (11% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 8000/60000                 (13% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 9600/60000                 (16% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 11200/60000                 (19% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 12800/60000                 (21% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 14400/60000                 (24% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 16000/60000                 (27% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 17600/60000                 (29% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 19200/60000                 (32% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 20800/60000                 (35% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 22400/60000                 (37% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 24000/60000                 (40% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 25600/60000                 (43% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 27200/60000                 (45% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 28800/60000                 (48% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 30400/60000                 (51% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 32000/60000                 (53% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 33600/60000                 (56% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 35200/60000                 (59% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 36800/60000                 (61% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 38400/60000                 (64% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 40000/60000                 (67% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 41600/60000                 (69% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 43200/60000                 (72% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 44800/60000                 (75% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 46400/60000                 (77% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 48000/60000                 (80% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 49600/60000                 (83% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 51200/60000                 (85% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 52800/60000                 (88% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 54400/60000                 (91% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 56000/60000                 (93% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 57600/60000                 (96% | Loss: 2.302585\n",
      "Train Epoch: 1 | Batch Status: 59200/60000                 (99% | Loss: 2.302585\n",
      "=======================\n",
      " Test set: Average loss: 0.1439, Accuracy: 0.098\n"
     ]
    }
   ],
   "source": [
    "!python3 run_cnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, download=True, transform=transforms.ToTensor())\n",
    "valid_dataset = datasets.MNIST(root='./mnist_data/', train=False, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, download=True, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "vaild_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "1 28 28\n"
     ]
    }
   ],
   "source": [
    "shape = train_dataset[0][0].shape\n",
    "print(shape)\n",
    "C = shape[0]\n",
    "W = shape[1]\n",
    "H = shape[2]\n",
    "print(C, W, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_func, train_loader, optimizer):\n",
    "    model.train()\n",
    "    for batch_index, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = loss_func(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_index % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch+1} | Batch Status: {batch_index*len(x)}/{len(train_loader.dataset)} \\\n",
    "            ({100. * batch_index * batch_size / len(train_loader.dataset):.0f}% | Loss: {loss.item():.6f}')\n",
    "\n",
    "def test(model, loss_func, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct_count = 0\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        test_loss += loss_func(y_pred, y).item()\n",
    "        pred = y_pred.data.max(1, keepdim=True)[1]\n",
    "        # torch.eq : Computes element-wise equality. return counts value\n",
    "        correct_count += pred.eq(y.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'=======================\\n Test set: Average loss: {test_loss:.4f}, Accuracy: {correct_count/len(test_loader.dataset):.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, C, W, H, K, S): # 채널, 너비, 높이, 커널 사이즈, 스트라이드\n",
    "        super(CNN, self).__init__()\n",
    "        # nn.Module에는 이미 conv 레이어가 구현되어 있다. \n",
    "        # 배치정규화도 구현되어있고 다 구현되어있습니다. \n",
    "        self.conv1 = nn.Conv2d(C, 32, kernel_size=K, stride=S)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=K, stride=S)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=K, stride=S)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size=K, stride=S):\n",
    "            print((size - (kernel_size - 1) - 1) // stride + 1)\n",
    "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        \n",
    "        convw = conv2d_size_out(W, K, S)\n",
    "        convw = conv2d_size_out(convw, K, S)\n",
    "        convw = conv2d_size_out(convw, K, S)\n",
    "        \n",
    "        self.linear_input_size = convw * convw * 128\n",
    "        self.fc = nn.Linear(self.linear_input_size, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1) # (batch_size, flatten_size)\n",
    "        x = F.relu(self.fc(x))\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "6\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN(C=C, W=W, H=H, K=3, S=2) \n",
    "cnn = cnn.to(device)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_272551/3935144841.py:30: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 | Batch Status: 0/60000             (0% | Loss: 2.372176\n",
      "Train Epoch: 1 | Batch Status: 3200/60000             (5% | Loss: 0.871514\n",
      "Train Epoch: 1 | Batch Status: 6400/60000             (11% | Loss: 0.683089\n",
      "Train Epoch: 1 | Batch Status: 9600/60000             (16% | Loss: 0.678688\n",
      "Train Epoch: 1 | Batch Status: 12800/60000             (21% | Loss: 0.480476\n",
      "Train Epoch: 1 | Batch Status: 16000/60000             (27% | Loss: 0.446314\n",
      "Train Epoch: 1 | Batch Status: 19200/60000             (32% | Loss: 0.415241\n",
      "Train Epoch: 1 | Batch Status: 22400/60000             (37% | Loss: 0.665354\n",
      "Train Epoch: 1 | Batch Status: 25600/60000             (43% | Loss: 0.698302\n",
      "Train Epoch: 1 | Batch Status: 28800/60000             (48% | Loss: 0.577258\n",
      "Train Epoch: 1 | Batch Status: 32000/60000             (53% | Loss: 0.872980\n",
      "Train Epoch: 1 | Batch Status: 35200/60000             (59% | Loss: 0.379692\n",
      "Train Epoch: 1 | Batch Status: 38400/60000             (64% | Loss: 0.683887\n",
      "Train Epoch: 1 | Batch Status: 41600/60000             (69% | Loss: 0.396851\n",
      "Train Epoch: 1 | Batch Status: 44800/60000             (75% | Loss: 0.724040\n",
      "Train Epoch: 1 | Batch Status: 48000/60000             (80% | Loss: 0.175095\n",
      "Train Epoch: 1 | Batch Status: 51200/60000             (85% | Loss: 0.222495\n",
      "Train Epoch: 1 | Batch Status: 54400/60000             (91% | Loss: 0.451018\n",
      "Train Epoch: 1 | Batch Status: 57600/60000             (96% | Loss: 0.388039\n",
      "Train Epoch: 2 | Batch Status: 0/60000             (0% | Loss: 0.235168\n",
      "Train Epoch: 2 | Batch Status: 3200/60000             (5% | Loss: 0.673967\n",
      "Train Epoch: 2 | Batch Status: 6400/60000             (11% | Loss: 0.633914\n",
      "Train Epoch: 2 | Batch Status: 9600/60000             (16% | Loss: 0.681743\n",
      "Train Epoch: 2 | Batch Status: 12800/60000             (21% | Loss: 0.465672\n",
      "Train Epoch: 2 | Batch Status: 16000/60000             (27% | Loss: 0.593200\n",
      "Train Epoch: 2 | Batch Status: 19200/60000             (32% | Loss: 0.384622\n",
      "Train Epoch: 2 | Batch Status: 22400/60000             (37% | Loss: 0.444509\n",
      "Train Epoch: 2 | Batch Status: 25600/60000             (43% | Loss: 0.510717\n",
      "Train Epoch: 2 | Batch Status: 28800/60000             (48% | Loss: 0.385733\n",
      "Train Epoch: 2 | Batch Status: 32000/60000             (53% | Loss: 0.434421\n",
      "Train Epoch: 2 | Batch Status: 35200/60000             (59% | Loss: 0.408630\n",
      "Train Epoch: 2 | Batch Status: 38400/60000             (64% | Loss: 0.281410\n",
      "Train Epoch: 2 | Batch Status: 41600/60000             (69% | Loss: 0.107265\n",
      "Train Epoch: 2 | Batch Status: 44800/60000             (75% | Loss: 0.442178\n",
      "Train Epoch: 2 | Batch Status: 48000/60000             (80% | Loss: 0.360702\n",
      "Train Epoch: 2 | Batch Status: 51200/60000             (85% | Loss: 0.473355\n",
      "Train Epoch: 2 | Batch Status: 54400/60000             (91% | Loss: 0.382948\n",
      "Train Epoch: 2 | Batch Status: 57600/60000             (96% | Loss: 0.458095\n",
      "Train Epoch: 3 | Batch Status: 0/60000             (0% | Loss: 0.506015\n",
      "Train Epoch: 3 | Batch Status: 3200/60000             (5% | Loss: 0.302263\n",
      "Train Epoch: 3 | Batch Status: 6400/60000             (11% | Loss: 0.655034\n",
      "Train Epoch: 3 | Batch Status: 9600/60000             (16% | Loss: 0.374789\n",
      "Train Epoch: 3 | Batch Status: 12800/60000             (21% | Loss: 0.817761\n",
      "Train Epoch: 3 | Batch Status: 16000/60000             (27% | Loss: 0.630377\n",
      "Train Epoch: 3 | Batch Status: 19200/60000             (32% | Loss: 0.796909\n",
      "Train Epoch: 3 | Batch Status: 22400/60000             (37% | Loss: 0.291389\n",
      "Train Epoch: 3 | Batch Status: 25600/60000             (43% | Loss: 0.579781\n",
      "Train Epoch: 3 | Batch Status: 28800/60000             (48% | Loss: 0.371296\n",
      "Train Epoch: 3 | Batch Status: 32000/60000             (53% | Loss: 0.448842\n",
      "Train Epoch: 3 | Batch Status: 35200/60000             (59% | Loss: 0.432953\n",
      "Train Epoch: 3 | Batch Status: 38400/60000             (64% | Loss: 0.583650\n",
      "Train Epoch: 3 | Batch Status: 41600/60000             (69% | Loss: 0.576457\n",
      "Train Epoch: 3 | Batch Status: 44800/60000             (75% | Loss: 0.369331\n",
      "Train Epoch: 3 | Batch Status: 48000/60000             (80% | Loss: 0.569095\n",
      "Train Epoch: 3 | Batch Status: 51200/60000             (85% | Loss: 0.033030\n",
      "Train Epoch: 3 | Batch Status: 54400/60000             (91% | Loss: 0.488533\n",
      "Train Epoch: 3 | Batch Status: 57600/60000             (96% | Loss: 0.350674\n",
      "Train Epoch: 4 | Batch Status: 0/60000             (0% | Loss: 0.719801\n",
      "Train Epoch: 4 | Batch Status: 3200/60000             (5% | Loss: 0.545721\n",
      "Train Epoch: 4 | Batch Status: 6400/60000             (11% | Loss: 0.434347\n",
      "Train Epoch: 4 | Batch Status: 9600/60000             (16% | Loss: 0.377354\n",
      "Train Epoch: 4 | Batch Status: 12800/60000             (21% | Loss: 0.362488\n",
      "Train Epoch: 4 | Batch Status: 16000/60000             (27% | Loss: 0.504408\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_272551/283131014.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mce_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mce_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_272551/688346569.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, model, loss_func, train_loader, optimizer)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             print(f'Train Epoch: {epoch+1} | Batch Status: {batch_index*len(x)}/{len(train_loader.dataset)} \\\n",
      "\u001b[0;32m~/Working/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Working/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Working/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    134\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Working/lib/python3.8/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train(epoch, cnn, ce_loss, train_loader, optimizer)\n",
    "\n",
    "test(cnn, ce_loss, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "026aceb1435085fcef523649cdfc9385a4a55dbc5c65435142607853821fa50a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('Working': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
