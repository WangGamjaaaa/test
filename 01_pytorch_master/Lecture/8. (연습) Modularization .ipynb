{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "epochs = 5\n",
    "kernel_size = 3\n",
    "stride = 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 모듈화  \n",
    "코드의 가독성을 좋게 하고 보수 및 관리를 쉽게 합니다.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```\n",
    "+-- configs/\n",
    "|   +-- cnn.yaml\n",
    "+-- dataset\n",
    "|   +-- mnist_data/...\n",
    "|   +-- MNIST_LOADER.py\n",
    "+-- models\n",
    "|   +-- cnn.py\n",
    "|   +-- res_net.py\n",
    "+-- runs\n",
    "|   +-- cnn/...\n",
    "|   +-- experiments/...\n",
    "+-- wetights\n",
    "|   +-- cnn.pth\n",
    "+-- argparse_tutorial.py\n",
    "+-- README.md\n",
    "+-- run_cnn.py\n",
    "+-- run_cnn2.py\n",
    "+-- requirements.txt\n",
    "+-- utils.py\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Quiz (Easy)  \n",
    "위 구조와 같이 모듈화를 진행하겠습니다.  \n",
    "1) dataset 폴더를 만들고 mnist_data 폴더를 옮겨주세요. 그리고 MNIST_LOADER.py를 dataset에 만듭니다.   \n",
    "2) models 폴더를 만들고 cnn.py, res_net.py를 만들고 내용을 채워주세요   \n",
    "3) utils 파일을 만들고 conv2d_size_out 함수를 작성하세요. CNN 클래스의 생성자에 정의된 함수를 그대로 옮기면 됩니다.  # 지금 안합니다.\n",
    "4) runs 폴더를 만들어주세요.   \n",
    "5) weights 폴더를 만들어주세요.   \n",
    "6) requirements.txt 파일을 만들어주세요.    "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, download=True, transform=transforms.ToTensor())\n",
    "valid_dataset = datasets.MNIST(root='./mnist_data/', train=False, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, download=True, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "vaild_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def train(epoch, model, loss_func, train_loader, optimizer):\n",
    "    model.train()\n",
    "    for batch_index, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = loss_func(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_index % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch+1} | Batch Status: {batch_index*len(x)}/{len(train_loader.dataset)} \\\n",
    "            ({100. * batch_index * batch_size / len(train_loader.dataset):.0f}% | Loss: {loss.item():.6f}')\n",
    "\n",
    "def test(model, loss_func, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct_count = 0\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        test_loss += loss_func(y_pred, y).item()\n",
    "        pred = y_pred.data.max(1, keepdim=True)[1]\n",
    "        # torch.eq : Computes element-wise equality. return counts value\n",
    "        correct_count += pred.eq(y.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'=======================\\n Test set: Average loss: {test_loss:.4f}, Accuracy: {correct_count/len(test_loader.dataset):.3}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# class CNN(nn.Module):\n",
    "#     def __init__(self, C, W, H, K, S): # 채널, 너비, 높이, 커널 사이즈, 스트라이드\n",
    "#         super(CNN, self).__init__()\n",
    "#         # nn.Module에는 이미 conv 레이어가 구현되어 있다. \n",
    "#         # 배치정규화도 구현되어있고 다 구현되어있습니다. \n",
    "#         self.conv1 = nn.Conv2d(C, 32, kernel_size=K, stride=S)\n",
    "#         self.bn1 = nn.BatchNorm2d(32)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=K, stride=S)\n",
    "#         self.bn2 = nn.BatchNorm2d(64)\n",
    "#         self.conv3 = nn.Conv2d(64, 128, kernel_size=K, stride=S)\n",
    "#         self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "#         def conv2d_size_out(size, kernel_size=K, stride=S):\n",
    "#             print((size - (kernel_size - 1) - 1) // stride + 1)\n",
    "#             return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        \n",
    "#         convw = conv2d_size_out(W, K, S)\n",
    "#         convw = conv2d_size_out(convw, K, S)\n",
    "#         convw = conv2d_size_out(convw, K, S)\n",
    "        \n",
    "#         self.linear_input_size = convw * convw * 128\n",
    "#         self.fc = nn.Linear(self.linear_input_size, 10)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.bn1(self.conv1(x)))\n",
    "#         x = F.relu(self.bn2(self.conv2(x)))\n",
    "#         x = F.relu(self.bn3(self.conv3(x)))\n",
    "#         x = x.view(x.size(0), -1) # (batch_size, flatten_size)\n",
    "#         x = F.relu(self.fc(x))\n",
    "#         return F.log_softmax(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "%%writefile run_cnn3.py\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from models.cnn import CNN\n",
    "from dataset.MNIST_LOADER import make_loader\n",
    "import argparse\n",
    "import yaml\n",
    "\n",
    "def train(epoch, model, loss_func, train_loader, optimizer):\n",
    "    model.train()\n",
    "    for batch_index, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = loss_func(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_index % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch+1} | Batch Status: {batch_index*len(x)}/{len(train_loader.dataset)} \\\n",
    "            ({100. * batch_index * batch_size / len(train_loader.dataset):.0f}% | Loss: {loss.item():.6f}')\n",
    "\n",
    "def test(model, loss_func, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct_count = 0\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        test_loss += loss_func(y_pred, y).item()\n",
    "        pred = y_pred.data.max(1, keepdim=True)[1]\n",
    "        # torch.eq : Computes element-wise equality. return counts value\n",
    "        correct_count += pred.eq(y.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'=======================\\n Test set: Average loss: {test_loss:.4f}, Accuracy: {correct_count/len(test_loader.dataset):.3}')\n",
    "\n",
    "# parser 정의\n",
    "parser = argparse.ArgumentParser(description='Argparse Tutorial')\n",
    "\n",
    "parser.add_argument('--config_path', type=str, default='configs/cnn.yaml', help = \"Score of english\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "train_loader, vaild_loader, test_loader, shape = make_loader(16)\n",
    "C = shape[0]\n",
    "W = shape[1]\n",
    "H = shape[2]\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "cnn = CNN(C=C, W=W, H=H, K=3, S=2) \n",
    "cnn = cnn.to(device)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# with 구문으로 파일을 불러옵니다.\n",
    "\n",
    "with open(args.config_path) as f:\n",
    "    config = yaml.load(f,Loader=yaml.FullLoader)\n",
    "    print(type(config))\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = config['batch_size']\n",
    "learning_rate = config['learning_rate']\n",
    "epochs = config['epochs']\n",
    "kernel_size = config['kernel_size']\n",
    "stride = config['stride']\n",
    "\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(epoch, cnn, ce_loss, train_loader, optimizer)\n",
    "\n",
    "test(cnn, ce_loss, test_loader)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting run_cnn3.py\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "!python3 run_cnn3.py"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "channel: 1, width: 28, height: 28\n",
      "13\n",
      "6\n",
      "2\n",
      "<class 'dict'>\n",
      "/home/hchang/Working/Python3/Pytorch-Master/Practice/models/cnn.py:36: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n",
      "Train Epoch: 1 | Batch Status: 0/60000             (0% | Loss: 2.345618\n",
      "Train Epoch: 1 | Batch Status: 1600/60000             (3% | Loss: 0.768841\n",
      "Train Epoch: 1 | Batch Status: 3200/60000             (5% | Loss: 0.523701\n",
      "Train Epoch: 1 | Batch Status: 4800/60000             (8% | Loss: 1.301229\n",
      "Train Epoch: 1 | Batch Status: 6400/60000             (11% | Loss: 1.063578\n",
      "Train Epoch: 1 | Batch Status: 8000/60000             (13% | Loss: 0.363229\n",
      "Train Epoch: 1 | Batch Status: 9600/60000             (16% | Loss: 1.022959\n",
      "Train Epoch: 1 | Batch Status: 11200/60000             (19% | Loss: 0.339684\n",
      "Train Epoch: 1 | Batch Status: 12800/60000             (21% | Loss: 0.050853\n",
      "Train Epoch: 1 | Batch Status: 14400/60000             (24% | Loss: 0.697979\n",
      "Train Epoch: 1 | Batch Status: 16000/60000             (27% | Loss: 0.869521\n",
      "Train Epoch: 1 | Batch Status: 17600/60000             (29% | Loss: 0.507146\n",
      "Train Epoch: 1 | Batch Status: 19200/60000             (32% | Loss: 0.340423\n",
      "Train Epoch: 1 | Batch Status: 20800/60000             (35% | Loss: 0.838008\n",
      "Train Epoch: 1 | Batch Status: 22400/60000             (37% | Loss: 0.446485\n",
      "Train Epoch: 1 | Batch Status: 24000/60000             (40% | Loss: 0.439861\n",
      "Train Epoch: 1 | Batch Status: 25600/60000             (43% | Loss: 0.436995\n",
      "Train Epoch: 1 | Batch Status: 27200/60000             (45% | Loss: 1.098710\n",
      "Train Epoch: 1 | Batch Status: 28800/60000             (48% | Loss: 0.748688\n",
      "Train Epoch: 1 | Batch Status: 30400/60000             (51% | Loss: 0.864708\n",
      "Train Epoch: 1 | Batch Status: 32000/60000             (53% | Loss: 0.492087\n",
      "Train Epoch: 1 | Batch Status: 33600/60000             (56% | Loss: 0.155941\n",
      "Train Epoch: 1 | Batch Status: 35200/60000             (59% | Loss: 0.188903\n",
      "Train Epoch: 1 | Batch Status: 36800/60000             (61% | Loss: 0.879002\n",
      "Train Epoch: 1 | Batch Status: 38400/60000             (64% | Loss: 0.573955\n",
      "Train Epoch: 1 | Batch Status: 40000/60000             (67% | Loss: 0.150989\n",
      "Train Epoch: 1 | Batch Status: 41600/60000             (69% | Loss: 0.189115\n",
      "Train Epoch: 1 | Batch Status: 43200/60000             (72% | Loss: 0.878227\n",
      "Train Epoch: 1 | Batch Status: 44800/60000             (75% | Loss: 0.290081\n",
      "Train Epoch: 1 | Batch Status: 46400/60000             (77% | Loss: 0.305344\n",
      "Train Epoch: 1 | Batch Status: 48000/60000             (80% | Loss: 0.670093\n",
      "Train Epoch: 1 | Batch Status: 49600/60000             (83% | Loss: 0.594960\n",
      "Train Epoch: 1 | Batch Status: 51200/60000             (85% | Loss: 0.288434\n",
      "Train Epoch: 1 | Batch Status: 52800/60000             (88% | Loss: 0.288992\n",
      "Train Epoch: 1 | Batch Status: 54400/60000             (91% | Loss: 0.981943\n",
      "Train Epoch: 1 | Batch Status: 56000/60000             (93% | Loss: 0.608047\n",
      "Train Epoch: 1 | Batch Status: 57600/60000             (96% | Loss: 0.734374\n",
      "Train Epoch: 1 | Batch Status: 59200/60000             (99% | Loss: 0.631794\n",
      "Train Epoch: 2 | Batch Status: 0/60000             (0% | Loss: 0.153858\n",
      "Train Epoch: 2 | Batch Status: 1600/60000             (3% | Loss: 0.432930\n",
      "Train Epoch: 2 | Batch Status: 3200/60000             (5% | Loss: 0.436161\n",
      "Train Epoch: 2 | Batch Status: 4800/60000             (8% | Loss: 0.311711\n",
      "Train Epoch: 2 | Batch Status: 6400/60000             (11% | Loss: 0.733967\n",
      "Train Epoch: 2 | Batch Status: 8000/60000             (13% | Loss: 0.162439\n",
      "Train Epoch: 2 | Batch Status: 9600/60000             (16% | Loss: 0.750072\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"run_cnn3.py\", line 78, in <module>\n",
      "    train(epoch, cnn, ce_loss, train_loader, optimizer)\n",
      "  File \"run_cnn3.py\", line 22, in train\n",
      "    loss.backward()\n",
      "  File \"/home/hchang/Working/lib/python3.8/site-packages/torch/_tensor.py\", line 307, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/home/hchang/Working/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 154, in backward\n",
      "    Variable._execution_engine.run_backward(\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('Working': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "026aceb1435085fcef523649cdfc9385a4a55dbc5c65435142607853821fa50a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}